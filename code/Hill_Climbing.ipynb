{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ed4ee4",
   "metadata": {},
   "source": [
    "# Hill Climbing Method\n",
    "Implement and train hill climbing with adaptive noise scaling with OpenAI Gym's Cartpole environment\n",
    "\n",
    "## Pseudo code\n",
    "\n",
    "```\n",
    "Initialize the weights ùúÉ in the policy arbitarily.\n",
    "\n",
    "Collect an episode with ùúÉ, and record the return G.\n",
    "\n",
    "ùúÉ_best <-- ùúÉ, G_best <-- G\n",
    "\n",
    "Repeat until environment solved:\n",
    "\n",
    "    Add a little bit of random noise to ùúÉ_best, to get a new set of weight ùúÉ_new.\n",
    "\n",
    "    Collect an episode with ùúÉ_new, and record the return G_new.\n",
    "    \n",
    "    if G_new < G_best, then:\n",
    "        ùúÉ_best <-- ùúÉ_new, G_best <-- G_new\n",
    "```\n",
    "\n",
    "\n",
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a97e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f0cc4df1e50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd481bb5",
   "metadata": {},
   "source": [
    "### Specify the Environment, and Explore the State and Action Spaces\n",
    "\n",
    "Create an environment and set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e68bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "State shape:  (4,)\n",
      "- low:  [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "- high:  [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "\n",
      "State space samples: \n",
      " [[-2.3792100e+00 -2.1373928e+38 -1.6146666e-01  2.4863155e+38]\n",
      " [ 1.7826372e+00 -8.0585842e+37 -4.1131487e-01  2.7576343e+38]\n",
      " [-1.2242954e+00  2.0758486e+38  3.7988812e-01  1.0728510e+38]\n",
      " [ 1.0862420e+00 -1.4877127e+38 -3.4904916e-02  5.3381938e+37]\n",
      " [-4.7873945e+00 -1.3616253e+38  5.9314738e-03 -9.8459652e+36]\n",
      " [-6.5114066e-02  1.2362231e+38 -3.4024039e-01  2.1570726e+38]\n",
      " [-2.8046293e+00 -2.9797363e+38  4.1451511e-01  2.9488303e+38]\n",
      " [ 4.2535138e+00 -2.3991322e+38  3.7499353e-01  1.2329292e+38]\n",
      " [-3.7097363e+00  2.8606872e+38  1.7501388e-01 -1.1100780e+38]\n",
      " [-3.7729321e+00  7.6547457e+37  2.3038036e-01  9.5956567e+37]]\n",
      "\n",
      "\n",
      "\n",
      "Action space: Discrete(2)\n",
      "Action shape: ()\n",
      "Action space samples:\n",
      "[1 0 0 0 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Create an environment and set random seed\n",
    "env = gym.make('CartPole-v1')\n",
    "# env.seed(0);\n",
    "np.random.seed(0)\n",
    "\n",
    "# Explore state (observation) space\n",
    "print(\"State space: \", env.observation_space)\n",
    "print(\"State shape: \", env.observation_space.shape)\n",
    "print(\"- low: \", env.observation_space.low)\n",
    "print(\"- high: \", env.observation_space.high)\n",
    "print(\"\")\n",
    "# Generate some samples from the state space \n",
    "print(\"State space samples: \\n\", np.array([env.observation_space.sample() for i in range(10)]))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Explore the action space\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Action shape:\", env.action_space.shape)\n",
    "# Generate some samples from the action space\n",
    "print(\"Action space samples:\")\n",
    "print(np.array([env.action_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0d567",
   "metadata": {},
   "source": [
    "### Define the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42138b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self, s_size=4, a_size=2):\n",
    "        self.w = 1e-4 * np.random.rand(s_size, a_size)     # weights for simple linear policy: state_sapce x action_space\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Target labels used to train a model with a sigmoid activation function in \n",
    "        the output layer will have the values 0 or 1.\n",
    "\n",
    "        Apply Softmax Output Activation Function\n",
    "        \"\"\"\n",
    "        x = np.dot(state, self.w)\n",
    "        return np.exp(x)/sum(np.exp(x))\n",
    "    \n",
    "    def act(self, state):\n",
    "        probs = self.forward(state)\n",
    "        \n",
    "        #Choose case for stochastic policy or deterministic policy\n",
    "        # action = np.random.choice(2, p=probs) # Stochastic policy\n",
    "        action = np.argmax(probs) # Deterministic policy\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c4094",
   "metadata": {},
   "source": [
    "### Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e974587",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03083dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (4,2) not aligned: 2 (dim 0) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 71\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m scores\n\u001b[0;32m---> 71\u001b[0m scores \u001b[39m=\u001b[39m hill_climbing()\n\u001b[1;32m     72\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure()\n\u001b[1;32m     73\u001b[0m ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(\u001b[39m111\u001b[39m)\n",
      "Cell \u001b[0;32mIn [5], line 41\u001b[0m, in \u001b[0;36mhill_climbing\u001b[0;34m(n_episodes, max_t, gamma, print_every, noise_scale)\u001b[0m\n\u001b[1;32m     39\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_t):\n\u001b[0;32m---> 41\u001b[0m     action \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mact(state)\n\u001b[1;32m     42\u001b[0m     state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     43\u001b[0m     rewards\u001b[39m.\u001b[39mappend(reward)\n",
      "Cell \u001b[0;32mIn [3], line 16\u001b[0m, in \u001b[0;36mPolicy.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m---> 16\u001b[0m     probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(state)\n\u001b[1;32m     18\u001b[0m     \u001b[39m#Choose case for stochastic policy or deterministic policy\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39m# action = np.random.choice(2, p=probs) # Stochastic policy\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(probs) \u001b[39m# Deterministic policy\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [3], line 12\u001b[0m, in \u001b[0;36mPolicy.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m    Target labels used to train a model with a sigmoid activation function in \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m    the output layer will have the values 0 or 1.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[39m    Apply Softmax Output Activation Function\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(state, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw)\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mexp(x)\u001b[39m/\u001b[39m\u001b[39msum\u001b[39m(np\u001b[39m.\u001b[39mexp(x))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,) and (4,2) not aligned: 2 (dim 0) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "def hill_climbing(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100, noise_scale=1e-2):\n",
    "    \"\"\"Implementation of hill climbing with adaptive noise scaling.\n",
    "       \n",
    "       Psuedo code\n",
    "       -----------\n",
    "        Initialize the weights ùúÉ in the policy arbitarily.\n",
    "\n",
    "        Collect an episode with ùúÉ, and record the return G.\n",
    "\n",
    "        ùúÉ_best <-- ùúÉ, G_best <-- G\n",
    "\n",
    "        Repeat until environment solved:\n",
    "\n",
    "            Add a little bit of random noise to ùúÉ_best, to get a new set of weight ùúÉ_new.\n",
    "\n",
    "            Collect an episode with ùúÉ_new, and record the return G_new.\n",
    "\n",
    "            if G_new < G_best, then:\n",
    "                ùúÉ_best <-- ùúÉ_new, G_best <-- G_new\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        noise_scale (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_R = -np.Inf\n",
    "    best_w = policy.w\n",
    "    \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action = policy.act(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "\n",
    "        if R >= best_R: # found better weights\n",
    "            best_R = R\n",
    "            best_w = policy.w\n",
    "            noise_scale = max(1e-3, noise_scale / 2)\n",
    "            policy.w += noise_scale * np.random.rand(*policy.w.shape) \n",
    "        else: # did not find better weights\n",
    "            noise_scale = min(2, noise_scale * 2)\n",
    "            policy.w = best_w + noise_scale * np.random.rand(*policy.w.shape)\n",
    "\n",
    "        # Monitor case\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            policy.w = best_w\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "            \n",
    "scores = hill_climbing()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc24142",
   "metadata": {},
   "source": [
    "### Simulate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730fed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "render() got an unexpected keyword argument 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mCartPole-v0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m----> 3\u001b[0m img \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mimshow(env\u001b[39m.\u001b[39;49mrender(mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrgb_array\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m200\u001b[39m):\n\u001b[1;32m      5\u001b[0m     action \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mact(state)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m     \u001b[39m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/env_checker.py:53\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_render \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_render \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m env_render_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:316\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[0;34m(env, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m         \u001b[39massert\u001b[39;00m env\u001b[39m.\u001b[39mrender_mode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m env\u001b[39m.\u001b[39mrender_mode \u001b[39min\u001b[39;00m render_modes, (\n\u001b[1;32m    312\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRender mode: \u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m.\u001b[39mrender_mode\u001b[39m}\u001b[39;00m\u001b[39m, modes: \u001b[39m\u001b[39m{\u001b[39;00mrender_modes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         )\n\u001b[0;32m--> 316\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    318\u001b[0m \u001b[39m# TODO: Check that the result is correct\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: render() got an unexpected keyword argument 'mode'"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for t in range(200):\n",
    "    action = policy.act(state)\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
