{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Test Random Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__credits__ = [\"Desmond N.A. Hammond\"]\n",
    "%matplotlib widget\n",
    "\n",
    "from os import path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.classic_control import utils\n",
    "from gym.error import DependencyNotInstalled\n",
    "from numba import jit\n",
    "from time import time\n",
    "import numpy as np\n",
    "from math import acos, pi\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from ipywidgets import interactive\n",
    "from matplotlib import animation\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "from IPython.display import HTML, Javascript\n",
    "from ipywidgets import IntSlider\n",
    "from time import sleep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMSMEnv(gym.Env):\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"console\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, parameters: Optional[dict] = None):\n",
    "        #Initialize motor and all necessary parameters   \n",
    "        if parameters is None:\n",
    "            self.Rs = np.array(0.5) \n",
    "            self.Ld = np.array(3.5e-3)\n",
    "            self.Lq = np.array(5e-3)\n",
    "            self.p = np.array(3.)\n",
    "            self.psi_f = np.array(0.33)\n",
    "            self.Bm = np.array(0.0028)\n",
    "            self.J = np.array(0.004)\n",
    "            self.max_speed = np.array(500.)\n",
    "            self.max_torque = np.array(10.)\n",
    "            self.max_voltage = np.array(800.)\n",
    "            self.max_current = np.array(500.)\n",
    "            self.dt = np.array(0.001)\n",
    "            self.simTime = np.array(1.)\n",
    "        else:\n",
    "            self.Rs = np.array(parameters.get(\"Rs\")) if \"Rs\" in parameters else np.array(0.5) \n",
    "            self.Ld = np.array(parameters.get(\"Ld\")) if \"Ld\" in parameters else np.array(3.5e-3)\n",
    "            self.Lq = np.array(parameters.get(\"Lq\")) if \"Lq\" in parameters else np.array(5e-3)\n",
    "            self.p = np.array(parameters.get(\"p\")) if \"p\" in parameters else np.array(3.)\n",
    "            self.psi_f = np.array(parameters.get(\"psi_f\")) if \"psi_f\" in parameters else np.array(0.33)\n",
    "            self.Bm = np.array(parameters.get(\"Bm\")) if \"Bm\" in parameters else np.array(0.0028)\n",
    "            self.J = np.array(parameters.get(\"J\")) if \"J\" in parameters else np.array(0.004)\n",
    "            self.max_speed = np.array(parameters.get(\"max_speed\")) if \"max_speed\" in parameters else np.array(500.)\n",
    "            self.max_torque = np.array(parameters.get(\"max_torque\")) if \"max_torque\" in parameters else np.array(10.)\n",
    "            self.max_voltage = np.array(parameters.get(\"max_voltage\")) if \"max_voltage\" in parameters else np.array(800.)\n",
    "            self.max_current = np.array(parameters.get(\"max_current\")) if \"max_current\" in parameters else np.array(500.)  \n",
    "            self.dt = np.array(parameters.get(\"sample_time\")) if \"sample_time\" in parameters else np.array(0.001)\n",
    "            self.simTime = np.array(parameters.get(\"simulation_time\")) if \"simulation_time\" in parameters else np.array(1.) \n",
    "            \n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        min_action = np.array([-self.max_voltage, -self.max_voltage], dtype=np.float32)\n",
    "        max_action = np.array([self.max_voltage, self.max_voltage], dtype=np.float32)\n",
    "\n",
    "        min_observation = np.array([-self.max_current, -self.max_current, -self.max_speed, np.array(0.)], dtype=np.float32)\n",
    "        max_observation = np.array([self.max_current, self.max_current, self.max_speed, 2*np.pi], dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Box(low=min_action, high=max_action, shape=(2,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=min_observation, high=max_observation, shape=(4,), dtype=np.float32)\n",
    "        self.t = t = np.linspace(0, self.simTime, int(self.simTime/self.dt))\n",
    "        self.sim_length = len(t)\n",
    "        \n",
    "        print('t', len(t))\n",
    "        # State variables\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "        \n",
    "    \n",
    "    def rungekutta4_step(self, diff_eqn, x0, dt):\n",
    "        k1 = diff_eqn(x0)\n",
    "        k2 = diff_eqn(x0 + k1 * dt/2.)\n",
    "        k3 = diff_eqn(x0 + k2 * dt/2.)\n",
    "        k4 = diff_eqn(x0 + k3 * dt)\n",
    "        return x0 + (dt/6.) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "        \n",
    "    def step(self, u, id_ref, iq_ref, omega_ref, theta_ref, T_load):\n",
    "        self.id_ref = np.array(id_ref)\n",
    "        self.iq_ref = np.array(iq_ref)\n",
    "        self.omega_ref = np.array(omega_ref)\n",
    "        self.theta_ref = np.array(theta_ref)\n",
    "\n",
    "        self.T_load = np.clip(T_load, -self.max_torque, self.max_torque)\n",
    "        self.vd, self.vq = np.clip(u, -self.max_voltage, self.max_voltage).tolist()\n",
    "        self.x_prev = np.array([self.id,self.iq,self.omega,self.theta])\n",
    "        \n",
    "        # Compute new states\n",
    "        self.id, self.iq, self.omega, self.theta = self.rungekutta4_step(diff_eqn=self.Xdot, x0=self.x_prev, dt=self.dt).tolist()\n",
    "        self.x_current = np.array([self.id,self.iq,self.omega,self.theta])\n",
    "        self.id_dot, self.iq_dot, self.omega_dot, self.theta_dot = self.Xdot(self.x_current).tolist()\n",
    "        self.lambda_d = self.Ld*self.id + self.psi_f\n",
    "        self.lambda_q = self.Lq*self.iq\n",
    "        self.Te = (3/2)*(self.p)*(self.lambda_d*self.iq - self.lambda_q*self.id)\n",
    "        \n",
    "        # if self.render_mode == \"human\":\n",
    "        #     self.render()\n",
    "        \n",
    "        # calculate error\n",
    "        self.id_error = self.id - self.id_ref\n",
    "        self.iq_error = self.iq - self.iq_ref\n",
    "        self.omega_error = self.omega - self.omega_ref \n",
    "        self.theta_error = self.omega - self.theta_ref\n",
    "\n",
    "        # calculate error\n",
    "        self.reward = -np.abs(self.id_error) -np.abs(self.omega_error)\n",
    "\n",
    "        # check if episode is done\n",
    "        done = self.step_count==self.sim_length-1\n",
    "        self.update_plot = True if not done else False\n",
    "        self.step_count += 1 if self.update_plot else -1\n",
    "        if done:\n",
    "            Javascript('document.querySelector(\".anim-buttons > button:nth-child(6)\").click()') \n",
    "        print(self.step_count)\n",
    "        # set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "\n",
    "        \n",
    "        return self.get_observations(), self.reward, done, info\n",
    "\n",
    "\n",
    "    def Xdot(self, xdot):\n",
    "        id_, iq_, omega_, theta_= xdot.tolist()\n",
    "\n",
    "        x_dot = np.array([self.vd/self.Ld - (self.Rs*id_)/self.Ld + (self.Lq*iq_*omega_*self.p)/self.Ld,\n",
    "                         self.vq/self.Lq - (self.Rs*iq_)/self.Lq - (omega_*self.p*self.psi_f)/self.Lq - (self.Ld*id_*omega_*self.p)/self.Lq,\n",
    "                         -(self.T_load + self.Bm*omega_ - (3*self.p*(iq_*(self.psi_f + self.Ld*id_) - self.Lq*id_*iq_))/2)/self.J,\n",
    "                         omega_])\n",
    "            \n",
    "        id_dot, iq_dot, omega_dot, theta_dot = x_dot.tolist()\n",
    "        \n",
    "        return np.array([id_dot, iq_dot, omega_dot, theta_dot])\n",
    "\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = None\n",
    "        self.update_plot = False\n",
    "        self.step_count = -1\n",
    "        self.time_elapsed = 0\n",
    "        self.dataset1 = np.zeros(shape=(int(self.simTime/self.dt),16))\n",
    "        self.dataset2 = np.zeros(shape=(int(self.simTime/self.dt),16))\n",
    "        # reset state variables\n",
    "        if options is None:\n",
    "            self.id_dot = np.array([0.])\n",
    "            self.iq_dot = np.array([0.])\n",
    "            self.omega_dot = np.array([0.])            \n",
    "            self.theta_dot = np.array([0.])            \n",
    "            self.id = np.array([0.])\n",
    "            self.iq = np.array([0.])\n",
    "            self.omega = np.array([0.])\n",
    "            self.theta = np.array([0.])\n",
    "        else:\n",
    "            self.id = options.get(\"id_init\") if \"id_init\" in options else np.array([0.])\n",
    "            self.iq = options.get(\"iq_init\") if \"iq_init\" in options else np.array([0.])\n",
    "            self.omega = options.get(\"omega_init\") if \"omega_init\" in options else np.array([0.])\n",
    "            self.theta = options.get(\"theta_init\") if \"theta_init\" in options else np.array([0.])\n",
    "            self.id_dot = np.array([0.])\n",
    "            self.iq_dot = np.array([0.])\n",
    "            self.omega_dot = np.array([0.])\n",
    "            self.theta_dot = np.array([0.])\n",
    "\n",
    "        return self.get_observations(), {}\n",
    "\n",
    "    def get_observations(self):\n",
    "        if self.update_plot:\n",
    "            # self.dataset1[self.step_count,:] = np.array([np.array(self.id), np.array(self.iq), np.array(self.omega), np.array(self.theta), np.array(self.id_dot), np.array(self.iq_dot),\n",
    "            #                                    np.array(self.omega_dot), np.array(self.theta_dot), np.array(self.id_error), np.array(self.iq_error), np.array(self.omega_error), \n",
    "            #                                    np.array(self.vd), np.array(self.Te), np.array(self.lambda_d), np.array(self.reward), np.array(self.vq)], dtype=object)\n",
    "            \n",
    "            # self.dataset2[self.step_count,:] = np.array([self.id_ref, self.iq_ref, self.omega_ref, self.theta_ref, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, \n",
    "            #                                 self.T_load, self.lambda_q, np.nan, np.nan], dtype=object).reshape(1,-1)\n",
    "            self.dataset1[self.step_count,:] = np.random.rand(16)            \n",
    "            self.dataset2[self.step_count,:] = np.random.rand(16)\n",
    "                    # Manipulation \n",
    "            if self.step_count%100 == 0:\n",
    "                self.animate.value = str(int(0.5+np.sin(self.step_count)*50))\n",
    "        # sensor measurement noise can be implemented here\n",
    "        observations = np.array([self.id, self.iq, self.omega], dtype=np.float32)\n",
    "        return observations\n",
    "   \n",
    "    def render(self):\n",
    "\n",
    "        #     if mode != 'console':\n",
    "        #     raise NotImplementedError()\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        # create a figure and axes\n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        num_subplots = 16\n",
    "        ax = [None]*num_subplots\n",
    "        line1 = [None]*num_subplots\n",
    "        line2 = [None]*num_subplots\n",
    "        label1 = ['id', 'iq', 'omega', 'theta', 'id_dot', 'iq_dot', 'omega_dot', 'theta_dot', 'id_error', 'iq_error', 'omega_error', 'vd', 'T_elec', 'lambda_d', 'reward', 'vq']\n",
    "        label2 = ['id_ref', 'iq_ref', 'omega_ref', 'theta_ref', '_', '_', '_', '_', '_', '_', '_', '_', 'T_load', 'lambda_q', '_', '_']\n",
    "\n",
    "        \n",
    "        for i in range(num_subplots):\n",
    "            # create axes\n",
    "            ax[i] = plt.subplot(4,4,i+1, autoscale_on=True)\n",
    "            ax[i].set_xlim(( 0, self.simTime) if i>=12 else False)          \n",
    "            ax[i].set_ylim()\n",
    "            ax[i].set_xlabel('Time') if i>=12 else ax[i].tick_params('x', labelbottom=False)\n",
    "            # ax[i].set_ylabel('Magnitude')\n",
    "            # ax[i].set_title('Phase Plane')\n",
    "            ax[i].grid(True)# create objects that will change in the animation. These are initially empty, and will be given new values for each frame in the animation.\n",
    "            line1[i], = ax[i].plot([], [], color='blue', lw=1, label=label1[i])     # ax.plot returns a list of 2D line objects\n",
    "            line2[i], = ax[i].plot([], [], '-', color='red', lw=1, label=label2[i])\n",
    "            ax[i].legend()\n",
    "\n",
    "\n",
    "        def init():\n",
    "            \"\"\"initialize animation\"\"\"\n",
    "            for i in range(num_subplots):\n",
    "                line1[i].set_data([], [])\n",
    "                line2[i].set_data([], [])\n",
    "            return tuple(line1 + line2)\n",
    "\n",
    "        def drawframe(n):\n",
    "            for i in range(num_subplots):\n",
    "                self.dataset1 = np.random.rand(len(self.t),16)            \n",
    "                self.dataset2 = (1/0.5*i)*np.random.rand(len(self.t),16)                \n",
    "                line1[i].set_data(self.t, self.dataset1[:,i])\n",
    "                line2[i].set_data(self.t, self.dataset2[:,i])\n",
    "            # plt.draw()\n",
    "            return tuple(line1 + line2)\n",
    "        \n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        def do_anim(self):\n",
    "            ani = animation.FuncAnimation(fig, drawframe, frames=20, interval=1, blit=True).to_jshtml(fps=20, default_mode='once')\n",
    "            # display animation and click play (the 6th button)\n",
    "            display(HTML(ani))\n",
    "            # Javascript('document.querySelector(\".anim-buttons > button:nth-child(6)\").click()')\n",
    "            print('Done')\n",
    "            return\n",
    "\n",
    "\n",
    "        # choose the interval based on dt and the time to animate one step\n",
    "        # t0 = time()\n",
    "        # drawframe(0)\n",
    "        # t1 = time()\n",
    "        # interval = 1000*self.dt - (t1 - t0)\n",
    "        # print(interval,'interval')\n",
    "\n",
    "        self.animate = IntSlider(description='animate')\n",
    "        self.animate.observe(do_anim, 'value')\n",
    "        # display(self.animate)\n",
    "\n",
    "        # # Manipulation \n",
    "        # self.animate.value = \"3\"\n",
    "        \n",
    " \n",
    "                \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PMSMEnv(\n",
    "    render_mode='human',\n",
    "    parameters = {            \n",
    "                    \"Rs\" : 0.5,\n",
    "                    \"Ld\" : 3.5e-3,\n",
    "                    \"Lq\" : 5e-3,\n",
    "                    \"p\" : 3.,\n",
    "                    \"psi_f\" : 0.33,\n",
    "                    \"Bm\" : 0.0028,\n",
    "                    \"J\" : 0.004,\n",
    "                    \"max_speed\" : 500.,\n",
    "                    \"max_torque\" : 10.,\n",
    "                    \"max_voltage\" : 800.,\n",
    "                    \"max_current\" : 500.,\n",
    "                    \"sample_time\" : 0.0002,\n",
    "                    \"simulation_time\" : 1.\n",
    "                }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample(), env.action_space.sample()\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    initial_conditions = {  \"id_init\": np.array([0.]),\n",
    "                            \"iq_init\": np.array([3.]),                            \n",
    "                            \"omega_init\": np.array([0.]),\n",
    "                            \"theta_init\": np.array([0.])\n",
    "                         }\n",
    "    state = env.reset(options=initial_conditions)\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        id_ref = 0\n",
    "        iq_ref = 0\n",
    "        omega_ref = 2\n",
    "        theta_ref = 2\n",
    "        T_load = 0\n",
    "\n",
    "        n_state, reward, done, info = env.step(action, id_ref, iq_ref, omega_ref, theta_ref, T_load)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots appear as a pop up window, chose the backend: 'gtk', 'inline', 'osx', 'qt', 'qt4', 'tk', 'wx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=15, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# state matrix\n",
    "a = g/(lp*(4.0/3 - mp/(mp+mk)))\n",
    "A = np.array([[0, 1, 0, 0],\n",
    "              [0, 0, a, 0],\n",
    "              [0, 0, 0, 1],\n",
    "              [0, 0, a, 0]])\n",
    "\n",
    "# input matrix\n",
    "b = -1/(lp*(4.0/3 - mp/(mp+mk)))\n",
    "B = np.array([[0], [1/mt], [0], [b]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "R = np.eye(2, dtype=int)          # choose R (weight for input)\n",
    "Q = 5*np.eye(4, dtype=int)        # choose Q (weight for state)\n",
    "\n",
    "# get riccati solver\n",
    "from scipy import linalg\n",
    "\n",
    "# solve ricatti equation\n",
    "P = linalg.solve_continuous_are(A, B, Q, R)\n",
    "\n",
    "# calculate optimal controller gain\n",
    "K = np.dot(np.linalg.inv(R),\n",
    "           np.dot(B.T, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_state_controller(K, x):\n",
    "    # feedback controller\n",
    "    u = -np.dot(K, x)   # u = -Kx\n",
    "    if u > 0:\n",
    "        return 1, u     # if force_dem > 0 -> move cart right\n",
    "    else:\n",
    "        return 0, u     # if force_dem <= 0 -> move cart left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env.env.seed(1)     # seed for reproducibility\n",
    "obs = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    \n",
    "    # get force direction (action) and force value (force)\n",
    "    action, force = apply_state_controller(K, obs)\n",
    "    \n",
    "    # absolute value, since 'action' determines the sign, F_min = -10N, F_max = 10N\n",
    "    abs_force = abs(float(np.clip(force, -10, 10)))\n",
    "    \n",
    "    # change magnitute of the applied force in CartPole\n",
    "    env.env.force_mag = abs_force\n",
    "\n",
    "    # apply action\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        print(f'Terminated after {i+1} iterations.')\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
