{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46632,"status":"ok","timestamp":1677185789120,"user":{"displayName":"Desmond Hammond","userId":"17685451081689258891"},"user_tz":-60},"id":"Tu44HsqWDWor","outputId":"c563e7dc-1076-4eac-deb4-4a6606eaf77a"},"outputs":[],"source":["try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","if IN_COLAB:\n","  # Mount the Google Drive at mount\n","  mount='/content/drive'\n","  print(\"Colab: mounting Google drive on \", mount)\n","\n","  drive.mount(mount)\n","\n","  # Switch to the directory on the Google Drive that you want to use\n","  import os\n","  drive_root = mount + \"/My Drive/Colab Notebooks/ML-for-IC-Die-Images\"\n","  \n","  # Create drive_root if it doesn't exist\n","  create_drive_root = True\n","  if create_drive_root:\n","    print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","    os.makedirs(drive_root, exist_ok=True)\n","  \n","  # Change to the directory\n","  print(\"\\nColab: Changing directory to \", drive_root)\n","  %cd $drive_root\n","  !pwd\n","\n","  !pip install -r requirements.txt\n","  !sudo apt-get autoremove\n","\n","\n","  from IPython.display import JSON\n","  from google.colab import output\n","  from subprocess import getoutput\n","  \n","  # @title jQuery Terminal's [Features](https://terminal.jcubic.pl/)\n","\n","  def shell(command):\n","    if command.startswith('cd'):\n","      path = command.strip().split(maxsplit=1)[1]\n","      os.chdir(path)\n","      return JSON([''])\n","    return JSON([getoutput(command)])\n","  output.register_callback('shell', shell)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\akone\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# %%\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import tensorflow as tf\n","import gc\n","import memory_profiler\n","import numpy\n","import cupy as np\n","import matplotlib.pyplot as plt\n","import PIL\n","import scipy\n","import more_itertools\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import sys\n","import cv2\n","import glob\n","import os\n","import time\n","from matplotlib import style\n","#style.use('classic')\n","from numpy import genfromtxt, asarray, savez_compressed, load\n","from sklearn.model_selection import train_test_split\n","from PIL import Image \n","from io import StringIO\n","import os\n","\n","import gc\n","import memory_profiler\n","import numpy\n","import cupy\n","import matplotlib.pyplot as plt\n","import PIL\n","import scipy\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import sys\n","import cv2\n","import glob\n","import os\n","import time\n","from tensorflow.keras.layers import Conv3D,Activation,Conv2D, ConvLSTM2D, MaxPooling2D, MaxPooling3D, BatchNormalization, Flatten, Input, Dense, GRU, Embedding, LSTM, SimpleRNN, Dropout, Bidirectional, TimeDistributed\n","from tensorflow.keras.models import Sequential, load_model, Model\n","from tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n","from tensorflow.keras.utils import plot_model, to_categorical, Sequence\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","from tensorflow import keras\n","from matplotlib import style\n","#style.use('classic')\n","from joblib import Parallel, delayed\n","from numpy import genfromtxt\n","from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n","from sklearn.model_selection import train_test_split\n","from PIL import Image \n","from scipy.signal import resample\n","from io import StringIO\n","import os\n","from sklearn.model_selection import train_test_split\n","import shutil\n","#plt.style.use('ggplot')\n","#matplotlib.use( 'tkagg' )\n","from scipy import signal\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import os\n","from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, RobustScaler, StandardScaler\n","from sklearn.metrics import mean_squared_error\n","from numpy import genfromtxt\n","from tqdm.notebook import tqdm_notebook\n","from sklearn.model_selection import train_test_split\n","import pylab as pl\n","import seaborn as sns\n","sns.set_style(\"ticks\",{'axes.grid' : True})\n","\n","from pathlib import Path\n","import shutil\n","\n","\n","import tensorflow.keras.backend as K\n","import tensorflow_addons as tfa\n","from tensorflow.keras import layers\n","from tensorflow import keras\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Sequential, load_model, Model\n","\n","try:\n","  IN_COLAB = True\n","  from google.colab import drive\n","  from tensorflow.keras.optimizers.legacy import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","except:\n","  IN_COLAB = False\n","  from tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","\n","from tensorflow.keras.optimizers.legacy import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n","from tensorflow.keras.utils import plot_model, to_categorical, normalize\n","from tensorflow.python.ops.numpy_ops import np_config\n","np_config.enable_numpy_behavior()\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: /device:GPU:0\n","Error trying to configure computing device.\n","name 'sess' is not defined\n","TensorFlow verison:  2.10.1\n","CUDA verison:  64_112\n","CUDNN verison:  64_8\n"]}],"source":["import tensorflow as tf\n","from tensorflow.python.platform import build_info as tf_build_info\n","import tensorflow.python.platform.build_info as build\n","from tensorflow.compat.v1.keras.backend import set_session\n","\n","try:\n","  # tf.debugging.experimental.enable_dump_debug_info('.', tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n","  # tf.debugging.set_log_device_placement(True)\n","  from tensorflow.python.client import device_lib\n","\n","  device_name = tf.test.gpu_device_name()\n","  if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","  print('Found GPU at: {}'.format(device_name))\n","\n","  config = tf.compat.v1.ConfigProto()\n","  config.gpu_options.allow_growth = True\n","  config.gpu_options.per_process_gpu_memory_fraction = 0.1\n","  if sess: tf.compat.v1.InteractiveSession.close() \n","  sess = tf.compat.v1.InteractiveSession(config=config)\n","  set_session(sess)\n","  print(device_lib.list_local_devices())\n","  gpus = tf.config.experimental.list_physical_devices('GPU')\n","  for gpu in gpus:\n","    try:\n","    \n","      tf.config.experimental.set_memory_growth(gpu, True)\n","      # Restrict TensorFlow to only use the first GPU\n","      tf.config.set_visible_devices(gpus[0], 'GPU')\n","      logical_gpus = tf.config.list_logical_devices('GPU')\n","      print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n","    except RuntimeError as e:\n","      # Visible devices must be set before GPUs have been initialized\n","      print(e)\n","\n","except Exception as error:\n","    print(\"Error trying to configure computing device.\")\n","    print(error)\n","\n","print(\"TensorFlow verison: \",tf.__version__)\n","print(\"CUDA verison: \", build.build_info['cuda_version'])\n","print(\"CUDNN verison: \", build.build_info['cudnn_version'])\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Set Tensorflow-GPU precision \n","Mixed precision is the use of both 16-bit and 32-bit floating-point types in a model during training to make it run faster and use less memory. "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n","Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n","  NVIDIA GeForce GTX 1060 with Max-Q Design, compute capability 6.1\n","See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n","If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"]},{"data":{"text/plain":["'float32'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# tf.keras.backend.floatx()\n","# tf.keras.backend.set_floatx('float16')\n","# tf.keras.backend.set_floatx('float32')\n","policy = tf.keras.mixed_precision.Policy('mixed_float16')\n","tf.keras.mixed_precision.set_global_policy(policy)\n","tf.keras.backend.floatx()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":7,"status":"error","timestamp":1677186296648,"user":{"displayName":"Desmond Hammond","userId":"17685451081689258891"},"user_tz":-60},"id":"_QUaBNmKC-cu","outputId":"d0c915b7-366f-43d0-af44-5fe37b0a73b4"},"outputs":[],"source":["# Change as you wish\n","if IN_COLAB:\n","  TRAIN_FOLDER = './data/Train'\n","  VALIDATION_FOLDER = './data/Validation'\n","  IMAGE_INPUT_FOLDER = './data/Image_Input'\n","  IMAGE_OUTPUT_FOLDER = './data/Image_Output'\n","else:\n","  TRAIN_FOLDER = './../data/Train'\n","  VALIDATION_FOLDER = './../data/Validation'\n","  IMAGE_INPUT_FOLDER = './../data/Image_Input'\n","  IMAGE_OUTPUT_FOLDER = './../data/Image_Output'\n","\n","\n","# myData = pd.read_csv(os.path.join(DATASET_FOLDER, PENDULUM_DATA))\n","# myData.round(decimals=6)\n","# myData = myData.astype(np.float32)\n","# myData = myData.astype(np.float16)\n","# myData.describe().transpose()\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Set the paths to your annotated image folders\n","input_image_path = os.path.join(IMAGE_INPUT_FOLDER)\n","output_image_path = os.path.join(IMAGE_INPUT_FOLDER)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"oDsGXIkhC-cu"},"outputs":[],"source":["# Get the list of image and label file paths\n","input_filenames = glob.glob(os.path.join(input_image_path,'*.jpg'))\n","output_filenames = glob.glob(os.path.join(output_image_path,'*.jpg'))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["False"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["check_ims = [in_name.split('\\\\')[1]==out_name.split('\\\\')[1] for in_name,out_name in zip(input_filenames, output_filenames)]\n","any(not x for x in check_ims) #Check if at least one image names is not corresponding "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["test_size = 0.1  #Specify test percentage\n","\n","# Split the image and label file paths into training and validation sets\n","train_image_paths, validation_image_paths, train_label_paths, validation_label_paths = train_test_split(input_filenames, output_filenames, test_size=0.2, shuffle=True)\n","\n","# Create the training and validation folders\n","train_folder_path = os.path.join(TRAIN_FOLDER)\n","validation_folder_path = os.path.join(VALIDATION_FOLDER)\n","\n","os.makedirs(os.path.join(train_folder_path, 'Input_Image'), exist_ok=True)\n","os.makedirs(os.path.join(train_folder_path, 'Output_Image'), exist_ok=True)\n","os.makedirs(os.path.join(validation_folder_path, 'Input_Image'), exist_ok=True)\n","os.makedirs(os.path.join(validation_folder_path, 'Output_Image'), exist_ok=True)\n","\n","# Copy the training images and labels to the training folder\n","for image_path, label_path in zip(train_image_paths, train_label_paths):\n","    image_name = os.path.basename(image_path)\n","    label_name = os.path.basename(label_path)\n","    shutil.copy(image_path, os.path.join(train_folder_path, 'Input_Image', image_name))\n","    shutil.copy(label_path, os.path.join(train_folder_path, 'Output_Image',label_name))\n","\n","# Copy the validation images and labels to the validation folder\n","for image_path, label_path in zip(validation_image_paths, validation_label_paths):\n","    image_name = os.path.basename(image_path)\n","    label_name = os.path.basename(label_path)\n","    shutil.copy(image_path, os.path.join(validation_folder_path, 'Input_Image', image_name))\n","    shutil.copy(label_path, os.path.join(validation_folder_path, 'Output_Image', label_name))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def resize_with_padding(image, target_size):\n","    \"\"\"\n","    Resizes an image with padding and maintains aspect ratio while keeping the background transparent.\n","    :param image: input image as a numpy array\n","    :param target_size: a tuple (width, height) representing the target size of the output image\n","    :return: the resized image as a numpy array\n","    \"\"\"\n","    h, w = image.shape[:2]\n","    target_w, target_h = target_size\n","\n","    # Calculate the scale factor and the new dimensions\n","    scale = min(target_w / w, target_h / h)\n","    new_w = int(w * scale)\n","    new_h = int(h * scale)\n","\n","    # Resize the image\n","    image_resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n","\n","    # Create a transparent background\n","    background = np.zeros((target_h, target_w, 4), dtype=np.uint8)\n","    background[:, :, 3] = 0  # set alpha channel to zero\n","\n","    # Calculate the coordinates to paste the resized image\n","    x = int((target_w - new_w) / 2)\n","    y = int((target_h - new_h) / 2)\n","\n","    # Paste the resized image onto the transparent background\n","    background[y:y+new_h, x:x+new_w, :3] = image_resized\n","    background[y:y+new_h, x:x+new_w, 3] = 255  # set alpha channel to 255 (fully opaque)\n","\n","    return background\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class CustomDataGenerator(Sequence):\n","    def __init__(self, input_paths, output_paths, batch_size, input_shape, num_classes=5, plot=False):\n","        self.input_paths = input_paths\n","        self.output_paths = output_paths\n","        self.batch_size = batch_size\n","        self.input_shape = input_shape\n","        self.plot = plot\n","        self.num_classes = num_classes\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.input_paths) / float(self.batch_size)))\n","\n","    def __getitem__(self, idx):\n","        input_batch_paths = self.input_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        output_batch_paths = self.output_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n","\n","        input_images = list(map(self.image_dataloader, input_batch_paths))\n","        output_images = list(map(self.image_dataloader, output_batch_paths))\n","\n","        return cupy.asnumpy(input_images)/255., to_categorical(cupy.asnumpy(output_images), self.num_classes)\n","        \n","\n","    def image_dataloader(self, image_path):\n","        # Load the input image\n","        image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = image.reshape(image.shape[0],image.shape[1],image.shape[2])\n","        # Resize the image with padding\n","        image = resize_with_padding(image, self.input_shape)\n","        # Add an alpha channel to the input image\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2RGBA)\n","        if self.plot:\n","            plt.imshow(image)\n","            plt.title('Image')\n","            plt.show()\n","        return image\n","        \n","    def resize_with_padding(self, image, target_size):\n","        \"\"\"\n","        Resizes an image with padding and maintains aspect ratio while keeping the background transparent.\n","        :param image: input image as a numpy array\n","        :param target_size: a tuple (width, height) representing the target size of the output image\n","        :return: the resized image as a numpy array\n","        \"\"\"\n","        h, w = image.shape[:2]\n","        target_w, target_h = target_size\n","\n","        # Calculate the scale factor and the new dimensions\n","        scale = min(target_w / w, target_h / h)\n","        new_w = int(w * scale)\n","        new_h = int(h * scale)\n","\n","        # Resize the image\n","        image_resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n","\n","        # Create a transparent background\n","        background = np.zeros((target_h, target_w, 4), dtype=np.uint8)\n","        background[:, :, 3] = 0  # set alpha channel to zero\n","\n","        # Calculate the coordinates to paste the resized image\n","        x = int((target_w - new_w) / 2)\n","        y = int((target_h - new_h) / 2)\n","\n","        # Paste the resized image onto the transparent background\n","        background[y:y+new_h, x:x+new_w, :3] = image_resized\n","        background[y:y+new_h, x:x+new_w, 3] = 255  # set alpha channel to 255 (fully opaque)\n","\n","        return background\n","\n","\n","        \n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["height, width = 100, 100\n","batch_size = 2\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"IndexError","evalue":"index 7 is out of bounds for axis 1 with size 1","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m     train_generator \u001b[39m=\u001b[39m CustomDataGenerator(train_image_paths, train_label_paths, batch_size, \n\u001b[0;32m      4\u001b[0m                                           input_shape\u001b[39m=\u001b[39m(height, width), num_classes\u001b[39m=\u001b[39mi)\n\u001b[0;32m      6\u001b[0m     validation_generator \u001b[39m=\u001b[39m CustomDataGenerator(validation_image_paths, validation_label_paths, batch_size, \n\u001b[0;32m      7\u001b[0m                                                input_shape\u001b[39m=\u001b[39m(height, width), num_classes\u001b[39m=\u001b[39mi)\n\u001b[1;32m----> 9\u001b[0m     input_train_batch, output_train_batch\u001b[39m=\u001b[39m\u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_generator))\n\u001b[0;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# print('x_train shape: ', input_train_batch.shape, 'x_train dtype:', input_train_batch.dtype)  \u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39m# print('y_train shape: ', output_train_batch.shape, 'y_train dtype:', output_train_batch.dtype)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m#     print(f\"Max categories: {i}\")\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m#     print(error)\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\data_utils.py:515\u001b[0m, in \u001b[0;36mSequence.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    514\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 515\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m (\u001b[39mself\u001b[39m[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m))):\n\u001b[0;32m    516\u001b[0m         \u001b[39myield\u001b[39;00m item\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\data_utils.py:515\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    514\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 515\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m (\u001b[39mself\u001b[39;49m[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m))):\n\u001b[0;32m    516\u001b[0m         \u001b[39myield\u001b[39;00m item\n","Cell \u001b[1;32mIn[11], line 20\u001b[0m, in \u001b[0;36mCustomDataGenerator.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m input_images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_dataloader, input_batch_paths))\n\u001b[0;32m     18\u001b[0m output_images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_dataloader, output_batch_paths))\n\u001b[1;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m cupy\u001b[39m.\u001b[39masnumpy(input_images)\u001b[39m/\u001b[39m\u001b[39m255.\u001b[39m, to_categorical(cupy\u001b[39m.\u001b[39;49masnumpy(output_images), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_classes)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\np_utils.py:73\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m n \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     72\u001b[0m categorical \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n, num_classes), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m---> 73\u001b[0m categorical[np\u001b[39m.\u001b[39;49marange(n), y] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     74\u001b[0m output_shape \u001b[39m=\u001b[39m input_shape \u001b[39m+\u001b[39m (num_classes,)\n\u001b[0;32m     75\u001b[0m categorical \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(categorical, output_shape)\n","\u001b[1;31mIndexError\u001b[0m: index 7 is out of bounds for axis 1 with size 1"]}],"source":["for i in range(1,500):\n","    # try:\n","        train_generator = CustomDataGenerator(train_image_paths, train_label_paths, batch_size, \n","                                              input_shape=(height, width), num_classes=i)\n","        \n","        validation_generator = CustomDataGenerator(validation_image_paths, validation_label_paths, batch_size, \n","                                                   input_shape=(height, width), num_classes=i)\n","\n","        input_train_batch, output_train_batch=next(iter(train_generator))\n","        print(i)\n","        # print('x_train shape: ', input_train_batch.shape, 'x_train dtype:', input_train_batch.dtype)  \n","        # print('y_train shape: ', output_train_batch.shape, 'y_train dtype:', output_train_batch.dtype)\n","\n","        # input_validation_batch, output_validation_batch=next(iter(validation_generator))\n","\n","        # print('x_val shape: ', input_validation_batch.shape, 'x_val dtype:', input_validation_batch.dtype)  \n","        # print('y_val shape: ', output_validation_batch.shape, 'y_val dtype:', output_validation_batch.dtype)\n","\n","    # except Exception as error:\n","    #     print(f\"Max categories: {i}\")\n","    #     print(error)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Define the input shape of the image\n","input_shape = (height, width, 4)\n","\n","# Define the encoder layers\n","inputs = Input(input_shape)\n","conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n","conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n","pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","\n","conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n","conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n","pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n","conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n","pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","\n","conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n","conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n","drop4 = Dropout(0.5)(conv4)\n","pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","# Define the decoder layers with skip connections\n","up5 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(pool4)\n","concat5 = concatenate([up5, drop4])\n","conv5 = Conv2D(512, 3, activation='relu', padding='same')(concat5)\n","conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n","\n","up6 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5)\n","concat6 = concatenate([up6, conv3])\n","conv6 = Conv2D(256, 3, activation='relu', padding='same')(concat6)\n","conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n","\n","up7 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)\n","concat7 = concatenate([up7, conv2])\n","conv7 = Conv2D(128, 3, activation='relu', padding='same')(concat7)\n","conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n","\n","up8 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)\n","concat8 = concatenate([up8, conv1])\n","conv8 = Conv2D(64, 3, activation='relu', padding='same')(concat8)\n","conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n","\n","# Define the output layer with softmax activation\n","output = Conv2D(num_classes, 1,  activation='softmax', padding='same')(conv8)\n","\n","# Define the model\n","model = Model(inputs=inputs, outputs=output)\n","\n","# Compile the model with appropriate loss and metrics\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Load the annotated image\n","annotated_image = load_annotated_image('path/to/annotated/image.png')\n","\n","# Convert the annotation into categorical labels\n","categorical_labels = to_categorical(annotated_image, num_classes)\n","model.fit(x_train, y_train, batch_size=32, epochs=50, validation_data=(x_val, y_val))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the model using the generators\n","model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=train_generator.samples // batch_size,\n","    epochs=50,\n","    validation_data=validation_generator,\n","    validation_steps=validation_generator.samples // batch_size)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","model.fit_generator(train_generator,\n","                    steps_per_epoch=len(train_image_paths) // batch_size,\n","                    epochs=epochs,\n","                    validation_data=validation_generator,\n","                    validation_steps=len(validation_image_paths) // batch_size)\n"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"}}},"nbformat":4,"nbformat_minor":0}
