{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46632,"status":"ok","timestamp":1677185789120,"user":{"displayName":"Desmond Hammond","userId":"17685451081689258891"},"user_tz":-60},"id":"Tu44HsqWDWor","outputId":"c563e7dc-1076-4eac-deb4-4a6606eaf77a"},"outputs":[],"source":["try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","if IN_COLAB:\n","  # Mount the Google Drive at mount\n","  mount='/content/drive'\n","  print(\"Colab: mounting Google drive on \", mount)\n","\n","  drive.mount(mount)\n","\n","  # Switch to the directory on the Google Drive that you want to use\n","  import os\n","  drive_root = mount + \"/My Drive/Colab Notebooks/ML-for-IC-Die-Images\"\n","  \n","  # Create drive_root if it doesn't exist\n","  create_drive_root = True\n","  if create_drive_root:\n","    print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","    os.makedirs(drive_root, exist_ok=True)\n","  \n","  # Change to the directory\n","  print(\"\\nColab: Changing directory to \", drive_root)\n","  %cd $drive_root\n","  !pwd\n","\n","  !pip install -r requirements.txt\n","  !sudo apt-get autoremove\n","\n","\n","  from IPython.display import JSON\n","  from google.colab import output\n","  from subprocess import getoutput\n","  \n","  # @title jQuery Terminal's [Features](https://terminal.jcubic.pl/)\n","\n","  def shell(command):\n","    if command.startswith('cd'):\n","      path = command.strip().split(maxsplit=1)[1]\n","      os.chdir(path)\n","      return JSON([''])\n","    return JSON([getoutput(command)])\n","  output.register_callback('shell', shell)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# %%\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import tensorflow as tf\n","import gc\n","import memory_profiler\n","import numpy\n","import cupy as np\n","import matplotlib.pyplot as plt\n","import PIL\n","import scipy\n","import more_itertools\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import sys\n","import cv2\n","import glob\n","import os\n","import time\n","from matplotlib import style\n","#style.use('classic')\n","from numpy import genfromtxt, asarray, savez_compressed, load\n","from sklearn.model_selection import train_test_split\n","from PIL import Image \n","from io import StringIO\n","import os\n","\n","import gc\n","import memory_profiler\n","import numpy\n","import cupy\n","import matplotlib.pyplot as plt\n","import PIL\n","import scipy\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import sys\n","import cv2\n","import glob\n","import os\n","import time\n","from tensorflow.keras.layers import Conv3D,Activation,Conv2D, ConvLSTM2D, MaxPooling2D, MaxPooling3D, BatchNormalization, Flatten, Input, Dense, GRU, Embedding, LSTM, SimpleRNN, Dropout, Bidirectional, TimeDistributed\n","from tensorflow.keras.models import Sequential, load_model, Model\n","from tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n","from tensorflow.keras.utils import plot_model, to_categorical\n","from tensorflow import keras\n","from matplotlib import style\n","#style.use('classic')\n","from joblib import Parallel, delayed\n","from numpy import genfromtxt\n","from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n","from sklearn.model_selection import train_test_split\n","from PIL import Image \n","from scipy.signal import resample\n","from io import StringIO\n","\n","#plt.style.use('ggplot')\n","#matplotlib.use( 'tkagg' )\n","from scipy import signal\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import os\n","from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, RobustScaler, StandardScaler\n","from sklearn.metrics import mean_squared_error\n","from numpy import genfromtxt\n","from tqdm.notebook import tqdm_notebook\n","from sklearn.model_selection import train_test_split\n","import pylab as pl\n","import seaborn as sns\n","sns.set_style(\"ticks\",{'axes.grid' : True})\n","\n","from pathlib import Path\n","import shutil\n","\n","\n","import tensorflow.keras.backend as K\n","import tensorflow_addons as tfa\n","from tensorflow.keras import layers\n","from tensorflow import keras\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Sequential, load_model, Model\n","\n","try:\n","  IN_COLAB = True\n","  from google.colab import drive\n","  from tensorflow.keras.optimizers.legacy import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","except:\n","  IN_COLAB = False\n","  from tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","\n","from tensorflow.keras.optimizers.legacy import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n","from tensorflow.keras.utils import plot_model, to_categorical, normalize\n","from tensorflow.python.ops.numpy_ops import np_config\n","np_config.enable_numpy_behavior()\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: /device:GPU:0\n","Error trying to configure computing device.\n","InteractiveSession.close() missing 1 required positional argument: 'self'\n","TensorFlow verison:  2.10.1\n","CUDA verison:  64_112\n","CUDNN verison:  64_8\n"]}],"source":["import tensorflow as tf\n","from tensorflow.python.platform import build_info as tf_build_info\n","import tensorflow.python.platform.build_info as build\n","from tensorflow.compat.v1.keras.backend import set_session\n","\n","try:\n","  # tf.debugging.experimental.enable_dump_debug_info('.', tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n","  # tf.debugging.set_log_device_placement(True)\n","  from tensorflow.python.client import device_lib\n","\n","  device_name = tf.test.gpu_device_name()\n","  if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","  print('Found GPU at: {}'.format(device_name))\n","\n","  config = tf.compat.v1.ConfigProto()\n","  config.gpu_options.allow_growth = True\n","  config.gpu_options.per_process_gpu_memory_fraction = 0.1\n","  if sess: tf.compat.v1.InteractiveSession.close() \n","  sess = tf.compat.v1.InteractiveSession(config=config)\n","  set_session(sess)\n","  print(device_lib.list_local_devices())\n","  gpus = tf.config.experimental.list_physical_devices('GPU')\n","  for gpu in gpus:\n","    try:\n","    \n","      tf.config.experimental.set_memory_growth(gpu, True)\n","      # Restrict TensorFlow to only use the first GPU\n","      tf.config.set_visible_devices(gpus[0], 'GPU')\n","      logical_gpus = tf.config.list_logical_devices('GPU')\n","      print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n","    except RuntimeError as e:\n","      # Visible devices must be set before GPUs have been initialized\n","      print(e)\n","\n","except Exception as error:\n","    print(\"Error trying to configure computing device.\")\n","    print(error)\n","\n","print(\"TensorFlow verison: \",tf.__version__)\n","print(\"CUDA verison: \", build.build_info['cuda_version'])\n","print(\"CUDNN verison: \", build.build_info['cudnn_version'])\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Set Tensorflow-GPU precision \n","Mixed precision is the use of both 16-bit and 32-bit floating-point types in a model during training to make it run faster and use less memory. "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["'float16'"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# tf.keras.backend.floatx()\n","# tf.keras.backend.set_floatx('float16')\n","# tf.keras.backend.set_floatx('float32')\n","policy = tf.keras.mixed_precision.Policy('mixed_float16')\n","tf.keras.mixed_precision.set_global_policy(policy)\n","tf.keras.backend.floatx()"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":7,"status":"error","timestamp":1677186296648,"user":{"displayName":"Desmond Hammond","userId":"17685451081689258891"},"user_tz":-60},"id":"_QUaBNmKC-cu","outputId":"d0c915b7-366f-43d0-af44-5fe37b0a73b4"},"outputs":[],"source":["# Change as you wish\n","if IN_COLAB:\n","  IMAGE_INPUT_FOLDER = './data/Image_Input'\n","  IMAGE_OUTPUT_FOLDER = './data/Image_Output'\n","else:\n","  IMAGE_INPUT_FOLDER = './../data/Image_Input'\n","  IMAGE_OUTPUT_FOLDER = './../data/Image_Output'\n","\n","\n","# myData = pd.read_csv(os.path.join(DATASET_FOLDER, PENDULUM_DATA))\n","# myData.round(decimals=6)\n","# myData = myData.astype(np.float32)\n","# myData = myData.astype(np.float16)\n","# myData.describe().transpose()\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["input_image_path = os.path.join(IMAGE_INPUT_FOLDER)\n","output_image_path = os.path.join(IMAGE_INPUT_FOLDER)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"oDsGXIkhC-cu"},"outputs":[],"source":["input_filenames = glob.glob(os.path.join(input_image_path,'*.jpg'))\n","output_filenames = glob.glob(os.path.join(output_image_path,'*.jpg'))"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["False"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["\n","check_ims = [in_name.split('\\\\')[1]==out_name.split('\\\\')[1] for in_name,out_name in zip(input_filenames, output_filenames)]\n","any(not x for x in check_ims) #Check if at least one image names is not corresponding "]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def resize_with_padding(image, target_size):\n","    \"\"\"\n","    Resizes an image with padding and maintains aspect ratio while keeping the background transparent.\n","    :param image: input image as a numpy array\n","    :param target_size: a tuple (width, height) representing the target size of the output image\n","    :return: the resized image as a numpy array\n","    \"\"\"\n","    h, w = image.shape[:2]\n","    target_w, target_h = target_size\n","\n","    # Calculate the scale factor and the new dimensions\n","    scale = min(target_w / w, target_h / h)\n","    new_w = int(w * scale)\n","    new_h = int(h * scale)\n","\n","    # Resize the image\n","    image_resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n","\n","    # Create a transparent background\n","    background = np.zeros((target_h, target_w, 4), dtype=np.uint8)\n","    background[:, :, 3] = 0  # set alpha channel to zero\n","\n","    # Calculate the coordinates to paste the resized image\n","    x = int((target_w - new_w) / 2)\n","    y = int((target_h - new_h) / 2)\n","\n","    # Paste the resized image onto the transparent background\n","    background[y:y+new_h, x:x+new_w, :3] = image_resized\n","    background[y:y+new_h, x:x+new_w, 3] = 255  # set alpha channel to 255 (fully opaque)\n","\n","    return background\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Kofi as bo"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/plain":["(0.0, 1.0, 0.0, 1.0)"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["def image_dataloader(image_path, size=(1000,1000), plot=False):\n","    # Load the input image\n","    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = image.reshape(image.shape[0],image.shape[1],image.shape[2])\n","    # Resize the image with padding\n","    image = resize_with_padding(image, (1000, 1000))\n","    # Add an alpha channel to the input image\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2RGBA)\n","    if plot:\n","        plt.imshow(image)\n","        plt.title('Image')\n","        plt.show()\n","    return image\n","    \n","\n","inputDataset = list(map(image_dataloader, input_filenames))\n","outputDataset = list(map(image_dataloader, output_filenames))\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["input_image_rescaled = cupy.asnumpy(inputDataset)/255.        \n","output_image_rescaled = cupy.asnumpy(outputDataset)/255."]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["(0.0, 1.0, 0.0, 1.0)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["input_image_rescaled.min(), input_image_rescaled.max(), output_image_rescaled.min(), output_image_rescaled.max()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Check image dimensions"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["((98, 1000, 1000, 4), (98, 1000, 1000, 4))"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["input_image_rescaled.shape, output_image_rescaled.shape"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (88, 1000, 1000, 4)\n","y_train shape: (88, 1000, 1000, 4)\n","x_test shape: (10, 1000, 1000, 4)\n","y_test shape: (10, 1000, 1000, 4)\n"]}],"source":["train_split = 0.9 # Use 90 percent of th images for training\n","\n","x_train, x_test, y_train, y_test = train_test_split(input_image_rescaled, input_image_rescaled, train_size=train_split, random_state=50)\n","\n","print(f'x_train shape: {x_train.shape}') \n","print(f'y_train shape: {y_train.shape}')\n","print(f'x_test shape: {x_test.shape}')  \n","print(f'y_test shape: {y_test.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %% [markdown]\n","# %store -r x_train\n","# %store -r x_test\n","# %store -r y_train\n","# %store -r y_test\n","# %% [markdown]\n","# (samples, time, rows, cols, channels)\n","# (None , 1, 360, 1080, 3) means that you have only one sample that is a sequence of 1 images.\n","\n","# %%\n","def batch_generator(batch_size, train=None, validation=None):\n","    \"\"\"\n","    Generator function for creating random batches of training-data.\n","    \"\"\"\n","    if train:\n","        num_samples = num_train\n","        x_samples = x_train\n","        y_samples = y_train\n","        print('using train samples')\n","    elif validation:\n","        num_samples = num_val\n","        x_samples = x_test[:num_samples]\n","        y_samples = y_test[:num_samples]\n","        print('using validation samples')\n","    else:\n","        num_samples = num_test\n","        x_samples = x_test[-num_samples:]\n","        y_samples = y_test[-num_samples:]\n","        print('using test samples')\n","    # Infinite loop.\n","    while True:\n","        # Allocate a new array for the batch of input-signals.\n","        x_shape = (batch_size, x_samples.shape[1],x_samples.shape[2],x_samples.shape[3])\n","        x_batch = numpy.empty(shape=x_shape,)\n","        #print(x_shape)\n","        \n","        # Allocate a new array for the batch of output-signals.\n","        y1_shape = (batch_size, percentage_class)\n","        y1_batch = numpy.empty(shape=y1_shape,)\n","        y2_shape = (batch_size, 1)\n","        y2_batch = numpy.empty(shape=y2_shape,)\n","            \n","        # Fill the batch with random sequences of data.\n","        for i in range(batch_size):\n","            \n","            # Get a random start-index.\n","            # This points somewhere into the training-data.\n","            idx = int(np.random.randint(num_samples - 1))\n","            \n","            # Copy the sequences of data starting at this index.\n","            x_batch[i] = x_samples[idx]\n","\n","            y1_batch[i] = np.asnumpy(y_samples[:,0][idx])\n","            y2_batch[i] = np.asnumpy(y_samples[:,1][idx])\n","            \n","\n","        x_batch=x_batch.reshape(batch_size, x_samples.shape[1],x_samples.shape[2],x_samples.shape[3])\n","        y_batch=[y1_batch, to_categorical(y2_batch, num_classes=location_class, dtype='float32')]\n","        \n","        #print(y_batch)\n","        yield (x_batch, y_batch)\n","\n","\n","# %%\n","train_generator = batch_generator(batch_size=batch_size, train=True, validation=True)\n","x_train_batch, y_train_batch=next(train_generator)\n","\n","print('x_train shape: ', x_train_batch.shape, 'x_train dtype:', x_train_batch.dtype)  \n","print('y_train[0] shape: ', y_train_batch[0].shape, 'y_train[0] dtype:', y_train_batch[0].dtype)\n","print('y_train[1] shape: ', y_train_batch[1].shape, 'y_train[1] dtype:', y_train_batch[1].dtype)\n","\n","\n","# %%\n","val_generator = batch_generator(batch_size=batch_size, train=False, validation=True)\n","x_val_batch, y_val_batch=next(val_generator)\n","\n","print('x_val shape: ', x_val_batch.shape, 'x_val dtype:', x_val_batch.dtype)  \n","print('y_val[0] shape: ', y_val_batch[0].shape, 'y_val[] dtype:', y_val_batch[0].dtype)\n","print('y_val[1] shape: ', y_val_batch[1].shape, 'y_val[] dtype:', y_val_batch[1].dtype)\n","\n","\n","# %%\n","test_generator = batch_generator(batch_size=batch_size, train=False, validation=False)\n","x_test_batch, y_test_batch=next(test_generator)\n","\n","print('x_test shape: ', x_test_batch.shape, 'x_test dtype:', x_test_batch.dtype)  \n","print('y_test[0] shape: ', y_test_batch[0].shape, 'y_test dtype:', y_test_batch[0].dtype)\n","print('y_test[1] shape: ', y_test_batch[1].shape, 'y_test dtype:', y_test_batch[1].dtype)\n","\n","# %% [markdown]\n","# #Prepare Validation data\n","# \n","# x_val_shape = (len(x_test), sequence_length, x_train.shape[1], x_train.shape[2], x_train.shape[3])\n","# x_val = numpy.empty(shape=x_val_shape)\n","# \n","# for i in range(len(x_test)):\n","#     x_val[i] = x_test[i]\n","# \n","# x_val = x_val.reshape(len(x_test), sequence_length, x_train.shape[1], x_train.shape[2],x_train.shape[3])\n","# \n","# validation_data = (x_val[:batch_size], y_test[:batch_size])\n","#  \n","# print('x_val shape: ', validation_data[0].shape)  \n","# print('y_val shape: ', validation_data[1].shape)\n","\n","# %%\n","optimizer = RMSprop(lr=1e-5)\n","momentum=0.25\n","datshape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n","datshape\n","\n","# %% [markdown]\n","# tf.keras.backend.clear_session()\n","# gc.collect()\n","# del model\n","# model = Sequential()\n","# \n","# #First Convolutional layer\n","# model.add(Conv2D(filters = 64, kernel_size = (3,3), input_shape = datshape, padding='same', activation='selu', data_format='channels_last'))\n","# model.add(MaxPooling2D(pool_size = (2,2)))\n","# #model.add(BatchNormalization())\n","# #model.add(Dropout(0.05))\n","# \n","# #Second Convolutional layer\n","# model.add(Conv2D(filters = 64, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n","# model.add(MaxPooling2D(pool_size = (2,2)))\n","# #model.add(BatchNormalization())\n","# #model.add(Dropout(0.05))\n","# \n","# #Third Convolutional layer\n","# model.add(Conv2D(filters = 128, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n","# model.add(MaxPooling2D(pool_size = (2,2)))\n","# #model.add(BatchNormalization())\n","# #model.add(Dropout(0.05))\n","# \n","# #Fourth Convolutional layer\n","# model.add(Conv2D(filters = 128, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n","# model.add(MaxPooling2D(pool_size = (2,2)))\n","# #model.add(BatchNormalization())\n","# #model.add(Dropout(0.05))\n","# \n","# #Fifth Convolutional layer\n","# model.add(Conv2D(filters = 256, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n","# model.add(MaxPooling2D(pool_size = (2,2)))\n","# #model.add(BatchNormalization())\n","# #model.add(Dropout(0.1))\n","# \n","# #Sixth Convolutional layer\n","# model.add(Conv2D(filters = 256, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n","# model.add(MaxPooling2D(pool_size = (2,2)))\n","# #model.add(BatchNormalization())\n","# #model.add(Dropout(0.1))\n","# \n","# #Seventh Convolutional layer\n","# model.add(Conv2D(filters = 256, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n","# model.add(MaxPooling2D(pool_size = (2,2)))\n","# #model.add(BatchNormalization())\n","# #model.add(Dropout(0.1))\n","# \n","# #Eighth Convolutional layer\n","# model.add(Conv2D(filters = 512, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n","# model.add(MaxPooling2D(pool_size = (2,2), strides=(2,2)))\n","# #model.add(BatchNormalization())\n","# #model.add(Dropout(0.1))\n","# \n","# \n","# #Flattening\n","# #model.add(BatchNormalization())\n","# model.add(Flatten())\n","# \n","# \n","# #Hidden Layer\n","# #model.add(Dense(units = 1024, activation='selu')) \n","# #model.add(Dropout(0.5))\n","# #model.add(BatchNormalization())\n","# \n","# #Hidden Layer\n","# model.add(Dense(units=4096, activation='selu'))\n","# model.add(Dropout(0.25))\n","# #model.add(BatchNormalization())\n","# \n","# #Hidden Layer\n","# model.add(Dense(units=4096, activation='selu'))\n","# model.add(Dropout(0.25))\n","# #model.add(BatchNormalization())\n","# \n","# #Output Layer\n","# model.add(Dense(percentage_class, activation='selu'))\n","# \n","# \n","# model.compile(loss='mse', optimizer=optimizer)\n","# \n","# model.summary()\n","\n","# %%\n","tf.keras.backend.clear_session()\n","gc.collect()\n","#del model\n","\n","def create_convnet(img_path='Interturn_AI_model_image.png'):\n","  \n","    image_input = Input(shape = datshape, name=\"3-phase_Scaleograms_input\")\n","    \n","    first_Conv2D = Conv2D(filters = 64, kernel_size = (3,3), input_shape = datshape, padding='same', activation='selu', data_format='channels_last')(image_input)\n","    first_Pooling = MaxPooling2D(pool_size = (2,2))(first_Conv2D)\n","    \n","    second_Conv2D = Conv2D(filters = 64, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(first_Pooling)\n","    second_Pooling = MaxPooling2D(pool_size = (2,2))(second_Conv2D)\n","    \n","    third_Conv2D = Conv2D(filters = 82, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(second_Pooling)\n","    third_Pooling = MaxPooling2D(pool_size = (2,2))(third_Conv2D)\n","    \n","    forth_Conv2D = Conv2D(filters = 82, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(third_Pooling)\n","    forth_Pooling = MaxPooling2D(pool_size = (2,2))(forth_Conv2D)\n","    \n","    fifth_Conv2D = Conv2D(filters = 128, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(forth_Pooling)\n","    fifth_Pooling = MaxPooling2D(pool_size = (2,2))(fifth_Conv2D)\n","    \n","    sixth_Conv2D = Conv2D(filters = 128, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(fifth_Pooling)\n","    sixth_Pooling = MaxPooling2D(pool_size = (2,2))(sixth_Conv2D)\n","    \n","    seventh_Conv2D = Conv2D(filters = 256, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(sixth_Pooling)\n","    seventh_Pooling = MaxPooling2D(pool_size = (2,2))(seventh_Conv2D)\n","    \n","    eighth_Conv2D = Conv2D(filters = 512, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(seventh_Pooling)\n","    eighth_Pooling = MaxPooling2D(pool_size = (2,2), strides=(2,2))(eighth_Conv2D)\n","      \n","    flattened_layer = Flatten()(eighth_Pooling)\n","    \n","    first_DNN_0 =  Dense(units=4096, activation='selu')(flattened_layer)\n","    first_Dropout_0 = Dropout(0.25)(first_DNN_0)\n","\n","    second_DNN_0 =  Dense(units=4096, activation='selu')(first_Dropout_0)\n","    second_Dropout_0 = Dropout(0.25)(second_DNN_0)\n","\n","    first_DNN_1 =  Dense(units=256, activation='selu')(second_Dropout_0)\n","    first_Dropout_1 = Dropout(0.25)(first_DNN_1)\n","\n","    second_DNN_1 =  Dense(units=256, activation='selu')(first_Dropout_1)\n","    second_Dropout_1 = Dropout(0.25)(second_DNN_1)\n","    out_1 =  Dense(percentage_class, activation='selu', name='Magnitude')(second_Dropout_0)\n","    \n","    \n","    out_2 =  Dense(location_class, activation='sigmoid', name='Location')(second_Dropout_1)\n","\n","        \n","    model = Model(inputs=image_input, outputs=[out_1, out_2], name='Inter-turn_Fault_Detection_AI_model')\n","    \n","    return model\n","\n","\n","model = create_convnet()\n","\n","model.compile(loss=['mean_squared_error','categorical_crossentropy'], optimizer=optimizer,)\n","\n","model.summary()\n","\n","# %% [markdown]\n","# ### Callback Functions\n","# \n","# During training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras.\n","# %% [markdown]\n","# \n","# This is the callback for writing checkpoints during training.\n","\n","# %%\n","path_checkpoint = r'C:\\Users\\hp\\iCloudDrive\\Final Year Project\\Python Stuff\\AI model\\Model_checkpoint\\Inter-turn_fault_detect_model_checkpoint_6.keras'\n","callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n","                                      monitor='val_loss',\n","                                      verbose=1,\n","                                      save_weights_only=True,\n","                                      save_best_only=True)\n","\n","# %% [markdown]\n","# This is the callback for stopping the optimization when performance worsens on the validation-set.\n","\n","# %%\n","callback_early_stopping = EarlyStopping(monitor='val_loss',\n","                                        patience=100, verbose=1)\n","\n","# %% [markdown]\n","# This is the callback for writing the TensorBoard log during training.\n","\n","# %%\n","folderNumber=3\n","folderCount = str(3)  \n","#folderCount=str(input(\"Enter the Session RUN number: \"))\n","NAME = 'run'+ folderCount\n","logdir = os.path.join(r'logs', NAME)\n","Address=str(os.path.join(r'E:\\Dropbox\\AI' ,logdir))\n","#if os.path.exists(Address):\n","    #os.remove(Address)\n","    #NAME = 'run'+ (folderCount+1)\n","    #logdir = os.path.join(r'logs', NAME)\n","    #Address=str(os.path.join(r'E:\\Dropbox\\AI' ,logdir))\n","    #if os.path.exists(Address):\n","    #raise Exception('folder exists')\n","\n","print(Address)\n","callback_tensorboard = TensorBoard(log_dir=Address,\n","                                   histogram_freq=0,\n","                                   write_graph=True,\n","                                   write_images=True)\n","\n","# %% [markdown]\n","# This callback reduces the learning-rate for the optimizer if the validation-loss has not improved since the last epoch (as indicated by `patience=0`). The learning-rate will be reduced by multiplying it with the given factor. We set a start learning-rate of 1e-3 above, so multiplying it by 0.1 gives a learning-rate of 1e-4. We don't want the learning-rate to go any lower than this.\n","\n","# %%\n","\n","callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n","                                       factor=0.1,\n","                                       min_lr=1e-6,\n","                                       patience=0,\n","                                       verbose=1)\n","\n","\n","# %%\n","callbacks = [callback_early_stopping,\n","             callback_checkpoint,\n","             callback_tensorboard,\n","             callback_reduce_lr]\n","\n","\n","# %%\n","filepath = r'C:\\Users\\hp\\iCloudDrive\\Final Year Project\\Python Stuff\\AI model\\Model_architecture\\Inter-turn_Fault_Detection_AI_model_6'\n","def train_model(resume, epochs, initial_epoch, batch_size,model):\n","    def fit_model():\n","        \n","        print(model.summary())\n","        history=model.fit(    train_generator, \n","                              steps_per_epoch=steps_per_epoch, \n","                              epochs=EPOCHS, \n","                              verbose=1, \n","                              callbacks=callbacks,\n","                              validation_data=val_generator, \n","                              validation_steps=train_validation_steps, \n","                              #validation_freq=1,\n","                              #class_weight=None, \n","                              #max_queue_size=10, \n","                              #workers=8, \n","                              #use_multiprocessing=False,\n","                              shuffle=True,) \n","                              #initial_epoch=initial_epoch)\n","        model.load_weights(path_checkpoint)            \n","        model.save(filepath)\n","        model.evaluate(test_generator, steps=test_validation_steps)\n","        \n","        return history\n","    \n","    if resume:\n","        try:\n","            #del model\n","            #model = load_model(filepath)\n","            model.load_weights(path_checkpoint)\n","            print(model.summary())\n","            print(\"Model loading....\")\n","            model.evaluate(test_generator, steps=test_validation_steps)\n","            \n","        except Exception as error:\n","            print(\"Error trying to load checkpoint.\")\n","            print(error)\n","\n","            \n","    def plot_train_history(history, title):\n","        loss = history.history['loss']\n","        val_loss = np.asnumpy(history.history['val_loss'])\n","        accuracy = 1-np.asnumpy(loss)\n","        val_accuracy = 1-np.asnumpy(val_loss)\n","        epochs = range(len(loss))\n","        plt.figure(figsize=(15,5))\n","        plt.plot(epochs, loss, label='training_loss') \n","        plt.plot(epochs, val_loss, label='validation_loss')\n","        plt.title(title)\n","        plt.legend()\n","        plt.show()\n","        plt.figure(figsize=(15,5))\n","        plt.plot(epochs, accuracy, label='training_accuracy') \n","        plt.plot(epochs, val_accuracy, label='validation_accuracy')\n","        plt.title(title)\n","        plt.legend()\n","        plt.show()\n","        \n","    # Training the Model\n","    history = fit_model()\n","    plot_train_history(history, 'Model Training History ')\n","    return\n","\n","\n","# %%\n","EPOCHS=2000\n","#steps_per_epoch = int((num_train/batch_size)/17)\n","train_model(resume=False, epochs=EPOCHS, initial_epoch=0, batch_size=batch_size, model=model)\n","\n","\n","# %%\n","model.load_weights(path_checkpoint)\n","model.evaluate(train_generator, steps=train_validation_steps)\n","model.evaluate(test_generator, steps=test_validation_steps)\n","\n","\n","# %%\n","y_pred = pd.DataFrame(model.predict(x_test))\n","y_pred = np.asnumpy(y_pred).reshape(-1,2)\n","y_pred = [np.asnumpy(list(y_pred[:,0])).reshape(-1,1) , np.asnumpy(list(y_pred[:,1])).reshape(-1,3)]\n","y_pred[1] = y_pred[1].argmax(axis=-1)\n","y_pred = [y_pred[0].reshape(len(y_pred[0]),1), y_pred[1].reshape(len(y_pred[1]),1)]\n","\n","y_pred_shape = (len(y_pred[1]), 1)\n","y_pred_0 = numpy.empty(shape=y_pred_shape,)\n","y_pred_1 = numpy.empty(shape=y_pred_shape, dtype='int32')\n","            \n","# Fill the batch with random sequences of data.\n","for i in range(len(y_pred[1])):\n","    y_pred_0[i] = np.asnumpy(y_pred[0][i])\n","    y_pred_1[i] = np.asnumpy(y_pred[1][i])\n","\n","Y_pred = numpy.concatenate([y_pred_0, y_pred_1], axis=1)\n","Y_pred.shape\n","\n","\n","# %%\n","y_true = y_test\n","y_true.shape\n","\n","\n","# %%\n","#style.use('classic')\n","title='Test Set Evaluation'\n","def plot_prediction():\n","        plt.figure(figsize=(30,10))\n","        plt.plot( y_true[:,0]*100, 'o', label='True values') \n","        plt.plot( Y_pred[:,0]*100, 'o',label='Predicted values')\n","        plt.grid()\n","        plt.title(title)\n","        plt.legend()\n","        plt.show()\n","        plt.figure(figsize=(30,10))\n","        plt.plot( y_true[:,1],  label='True values') \n","        plt.plot( Y_pred[:,1],  label='Predicted values')\n","        plt.grid()\n","        plt.title(title)\n","        plt.legend()\n","        plt.show()\n","        return\n","\n","plot_prediction()\n","\n","\n","# %%\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n","\n","# Define the input shape of the image\n","input_shape = (height, width, 4)\n","\n","# Define the encoder layers\n","inputs = Input(input_shape)\n","conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n","conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n","pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","\n","conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n","conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n","pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n","conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n","pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","\n","conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n","conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n","drop4 = Dropout(0.5)(conv4)\n","pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","# Define the decoder layers with skip connections\n","up5 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(pool4)\n","concat5 = concatenate([up5, drop4])\n","conv5 = Conv2D(512, 3, activation='relu', padding='same')(concat5)\n","conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n","\n","up6 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5)\n","concat6 = concatenate([up6, conv3])\n","conv6 = Conv2D(256, 3, activation='relu', padding='same')(concat6)\n","conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n","\n","up7 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)\n","concat7 = concatenate([up7, conv2])\n","conv7 = Conv2D(128, 3, activation='relu', padding='same')(concat7)\n","conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n","\n","up8 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)\n","concat8 = concatenate([up8, conv1])\n","conv8 = Conv2D(64, 3, activation='relu', padding='same')(concat8)\n","conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n","\n","# Define the output layer with softmax activation\n","output = Conv2D(num_classes, 1,  activation='softmax', padding='same')(conv8)\n","\n","# Define the model\n","model = Model(inputs=inputs, outputs=output)\n","\n","# Compile the model with appropriate loss and metrics\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical\n","\n","# Load the annotated image\n","annotated_image = load_annotated_image('path/to/annotated/image.png')\n","\n","# Convert the annotation into categorical labels\n","categorical_labels = to_categorical(annotated_image, num_classes)\n","model.fit(x_train, y_train, batch_size=32, epochs=50, validation_data=(x_val, y_val))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","height, width = 500, 500\n","batch_size = 2\n","\n","# Define the data generator with appropriate preprocessing steps\n","datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    shear_range=0.0,\n","    zoom_range=0.0,\n","    horizontal_flip=True,\n","    validation_split=0.1)\n","\n","\n","\n","# Train the model using the generators\n","model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=train_generator.samples // batch_size,\n","    epochs=50,\n","    validation_data=validation_generator,\n","    validation_steps=validation_generator.samples // batch_size)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["import numpy as np\n","import cv2\n","import os\n","from tensorflow.keras.utils import Sequence\n","\n","class CustomDataGenerator(Sequence):\n","    def __init__(self, input_paths, output_paths, batch_size, input_shape):\n","        self.input_paths = input_paths\n","        self.output_paths = output_paths\n","        self.batch_size = batch_size\n","        self.input_shape = input_shape\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.input_paths) / float(self.batch_size)))\n","\n","    def __getitem__(self, idx):\n","        input_batch_paths = self.input_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        output_batch_paths = self.output_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n","\n","        input_images = list(map(image_dataloader, input_batch_paths))\n","        output_images = list(map(image_dataloader, output_batch_paths))\n","\n","        return cupy.asnumpy(input_images), cupy.asnumpy(output_images)\n","\n","    def image_dataloader(self, image_path, plot=False):\n","        # Load the input image\n","        image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = image.reshape(image.shape[0],image.shape[1],image.shape[2])\n","        # Resize the image with padding\n","        image = resize_with_padding(image, self.input_shape)\n","        # Add an alpha channel to the input image\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2RGBA)\n","        if plot:\n","            plt.imshow(image)\n","            plt.title('Image')\n","            plt.show()\n","        return image\n","        \n","    def resize_with_padding(self, image, target_size):\n","        \"\"\"\n","        Resizes an image with padding and maintains aspect ratio while keeping the background transparent.\n","        :param image: input image as a numpy array\n","        :param target_size: a tuple (width, height) representing the target size of the output image\n","        :return: the resized image as a numpy array\n","        \"\"\"\n","        h, w = image.shape[:2]\n","        target_w, target_h = target_size\n","\n","        # Calculate the scale factor and the new dimensions\n","        scale = min(target_w / w, target_h / h)\n","        new_w = int(w * scale)\n","        new_h = int(h * scale)\n","\n","        # Resize the image\n","        image_resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n","\n","        # Create a transparent background\n","        background = np.zeros((target_h, target_w, 4), dtype=np.uint8)\n","        background[:, :, 3] = 0  # set alpha channel to zero\n","\n","        # Calculate the coordinates to paste the resized image\n","        x = int((target_w - new_w) / 2)\n","        y = int((target_h - new_h) / 2)\n","\n","        # Paste the resized image onto the transparent background\n","        background[y:y+new_h, x:x+new_w, :3] = image_resized\n","        background[y:y+new_h, x:x+new_w, 3] = 255  # set alpha channel to 255 (fully opaque)\n","\n","        return background\n","\n","\n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_generator = CustomDataGenerator(train_image_paths, train_label_paths, batch_size, input_shape)\n","validation_generator = CustomDataGenerator(validation_image_paths, validation_label_paths, batch_size, input_shape)\n","\n","model.fit_generator(train_generator,\n","                    steps_per_epoch=len(train_image_paths) // batch_size,\n","                    epochs=epochs,\n","                    validation_data=validation_generator,\n","                    validation_steps=len(validation_image_paths) // batch_size)\n"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"}}},"nbformat":4,"nbformat_minor":0}
