{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67074,"status":"ok","timestamp":1677211923346,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"Tu44HsqWDWor","outputId":"4a1c816a-4f26-4650-9b90-3e68c0f2cbff"},"outputs":[{"name":"stdout","output_type":"stream","text":["We're running Colab\n","Colab: mounting Google drive on  /content/drive\n","Mounted at /content/drive\n","\n","Colab: making sure  /content/drive/My Drive/Colab Notebooks/ML-for-IC-Die-Images  exists.\n","\n","Colab: Changing directory to  /content/drive/My Drive/Colab Notebooks/ML-for-IC-Die-Images\n","/content/drive/.shortcut-targets-by-id/173HPda4lyToru4jXmvjCNmQmA4yuOTIg/ML-for-IC-Die-Images\n","/content/drive/.shortcut-targets-by-id/173HPda4lyToru4jXmvjCNmQmA4yuOTIg/ML-for-IC-Die-Images\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n","Collecting asttokens==2.2.1\n","  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.6.3)\n","Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (0.2.0)\n","Requirement already satisfied: cachetools==5.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (5.3.0)\n","Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (2022.12.7)\n","Requirement already satisfied: charset-normalizer==3.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (3.0.1)\n","Collecting colorama==0.4.6\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting comm==0.1.2\n","  Downloading comm-0.1.2-py3-none-any.whl (6.5 kB)\n","Collecting cupy-cuda112==10.6.0\n","  Downloading cupy_cuda112-10.6.0-cp38-cp38-manylinux1_x86_64.whl (80.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting debugpy==1.6.6\n","  Downloading debugpy-1.6.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting decorator==5.1.1\n","  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Collecting executing==1.2.0\n","  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: fastrlock==0.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (0.8.1)\n","Requirement already satisfied: flatbuffers==23.1.21 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (23.1.21)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (0.4.0)\n","Collecting google-auth==2.16.0\n","  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 18)) (0.4.6)\n","Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 19)) (0.2.0)\n","Requirement already satisfied: grpcio==1.51.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 20)) (1.51.1)\n","Collecting h5py==3.8.0\n","  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna==3.4\n","  Downloading idna-3.4-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ipykernel==6.21.2\n","  Downloading ipykernel-6.21.2-py3-none-any.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ipython==8.10.0\n","  Downloading ipython-8.10.0-py3-none-any.whl (784 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m784.3/784.3 KB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jedi==0.18.2\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 26)) (1.2.0)\n","Collecting jupyter_client==8.0.2\n","  Downloading jupyter_client-8.0.2-py3-none-any.whl (103 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jupyter_core==5.2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 28)) (5.2.0)\n","Requirement already satisfied: keras==2.11.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 29)) (2.11.0)\n","Collecting Keras-Preprocessing==1.1.2\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang==15.0.6.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 31)) (15.0.6.1)\n","Requirement already satisfied: Markdown==3.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 32)) (3.4.1)\n","Collecting MarkupSafe==2.1.2\n","  Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Collecting matplotlib-inline==0.1.6\n","  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n","Collecting memory-profiler==0.61.0\n","  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: more-itertools==9.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 36)) (9.0.0)\n","Collecting nest-asyncio==1.5.6\n","  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n","Collecting numpy==1.24.2\n","  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 39)) (3.2.2)\n","Collecting opencv-python==4.7.0.68\n","  Downloading opencv_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 41)) (3.3.0)\n","Requirement already satisfied: packaging==23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 42)) (23.0)\n","Requirement already satisfied: parso==0.8.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 43)) (0.8.3)\n","Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 44)) (0.7.5)\n","Collecting Pillow==9.4.0\n","  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs==3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 46)) (3.0.0)\n","Collecting prompt-toolkit==3.0.36\n","  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 KB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf==3.19.6 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 48)) (3.19.6)\n","Collecting psutil==5.9.4\n","  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pure-eval==0.2.2\n","  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n","Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 51)) (0.4.8)\n","Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 52)) (0.2.8)\n","Collecting Pygments==2.14.0\n","  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 54)) (2.8.2)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==305 (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==305\u001b[0m\u001b[31m\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following packages will be REMOVED:\n","  libnvidia-common-510\n","0 upgraded, 0 newly installed, 1 to remove and 21 not upgraded.\n","After this operation, 35.8 kB disk space will be freed.\n","(Reading database ... 128126 files and directories currently installed.)\n","Removing libnvidia-common-510 (510.108.03-0ubuntu1) ...\n"]}],"source":["try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","if IN_COLAB:\n","  # Mount the Google Drive at mount\n","  mount='/content/drive'\n","  print(\"Colab: mounting Google drive on \", mount)\n","\n","  drive.mount(mount)\n","\n","  # Switch to the directory on the Google Drive that you want to use\n","  import os\n","  drive_root = mount + \"/My Drive/Colab Notebooks/ML-for-IC-Die-Images\"\n","  \n","  # Create drive_root if it doesn't exist\n","  create_drive_root = True\n","  if create_drive_root:\n","    print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","    os.makedirs(drive_root, exist_ok=True)\n","  \n","  # Change to the directory\n","  print(\"\\nColab: Changing directory to \", drive_root)\n","  %cd $drive_root\n","  !pwd\n","\n","  !pip install -r requirements.txt\n","  !sudo apt-get autoremove\n","\n","\n","  from IPython.display import JSON\n","  from google.colab import output\n","  from subprocess import getoutput\n","  \n","  # @title jQuery Terminal's [Features](https://terminal.jcubic.pl/)\n","\n","  def shell(command):\n","    if command.startswith('cd'):\n","      path = command.strip().split(maxsplit=1)[1]\n","      os.chdir(path)\n","      return JSON([''])\n","    return JSON([getoutput(command)])\n","  output.register_callback('shell', shell)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":3106,"status":"error","timestamp":1677211926443,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"xaSwrEmAnbkX","outputId":"eb8f8876-9338-44de-bdf7-e6ed949226b1"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-32fb3b5757f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmemory_profiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'memory_profiler'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["# %%\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import tensorflow as tf\n","import gc\n","import memory_profiler\n","import numpy\n","import cupy as np\n","import matplotlib.pyplot as plt\n","import PIL\n","import scipy\n","import more_itertools\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import sys\n","import cv2\n","import glob\n","import os\n","import time\n","from matplotlib import style\n","#style.use('classic')\n","from numpy import genfromtxt, asarray, savez_compressed, load\n","from sklearn.model_selection import train_test_split\n","from PIL import Image \n","from io import StringIO\n","import os\n","\n","import gc\n","import memory_profiler\n","import numpy\n","import cupy\n","import matplotlib.pyplot as plt\n","import PIL\n","import scipy\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import sys\n","import cv2\n","import glob\n","import os\n","import time\n","from tensorflow.keras.layers import Conv3D,Activation,Conv2D,Cropping2D, ConvLSTM2D, MaxPooling2D, MaxPooling3D, BatchNormalization, Flatten, Input, Dense, GRU, Embedding, LSTM, SimpleRNN, Dropout, Bidirectional, TimeDistributed\n","from tensorflow.keras.models import Sequential, load_model, Model\n","from tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n","from tensorflow.keras.utils import plot_model, to_categorical, Sequence\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","from tensorflow import keras\n","from matplotlib import style\n","#style.use('classic')\n","from joblib import Parallel, delayed\n","from numpy import genfromtxt\n","from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n","from sklearn.model_selection import train_test_split\n","from PIL import Image \n","from scipy.signal import resample\n","from io import StringIO\n","import os\n","from sklearn.model_selection import train_test_split\n","import shutil\n","#plt.style.use('ggplot')\n","#matplotlib.use( 'tkagg' )\n","from scipy import signal\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import os\n","from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, RobustScaler, StandardScaler\n","from sklearn.metrics import mean_squared_error\n","from numpy import genfromtxt\n","from tqdm.notebook import tqdm_notebook\n","from sklearn.model_selection import train_test_split\n","import pylab as pl\n","import seaborn as sns\n","sns.set_style(\"ticks\",{'axes.grid' : True})\n","\n","from pathlib import Path\n","import shutil\n","\n","\n","import tensorflow.keras.backend as K\n","import tensorflow_addons as tfa\n","from tensorflow.keras import layers\n","from tensorflow import keras\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Sequential, load_model, Model\n","\n","try:\n","  IN_COLAB = True\n","  from google.colab import drive\n","  from tensorflow.keras.optimizers.legacy import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","except:\n","  IN_COLAB = False\n","  from tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","\n","from tensorflow.keras.optimizers.legacy import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n","from tensorflow.keras.utils import plot_model, to_categorical, normalize\n","from tensorflow.python.ops.numpy_ops import np_config\n","np_config.enable_numpy_behavior()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYmXzg1FnbkY","outputId":"5772996f-ed48-4c43-a518-27996d99c070"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: /device:GPU:0\n","Error trying to configure computing device.\n","name 'sess' is not defined\n","TensorFlow verison:  2.10.1\n","CUDA verison:  64_112\n","CUDNN verison:  64_8\n"]}],"source":["import tensorflow as tf\n","from tensorflow.python.platform import build_info as tf_build_info\n","import tensorflow.python.platform.build_info as build\n","from tensorflow.compat.v1.keras.backend import set_session\n","\n","try:\n","  # tf.debugging.experimental.enable_dump_debug_info('.', tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n","  # tf.debugging.set_log_device_placement(True)\n","  from tensorflow.python.client import device_lib\n","\n","  device_name = tf.test.gpu_device_name()\n","  if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","  print('Found GPU at: {}'.format(device_name))\n","\n","  config = tf.compat.v1.ConfigProto()\n","  config.gpu_options.allow_growth = True\n","  config.gpu_options.per_process_gpu_memory_fraction = 0.1\n","  if sess: tf.compat.v1.InteractiveSession.close() \n","  sess = tf.compat.v1.InteractiveSession(config=config)\n","  set_session(sess)\n","  print(device_lib.list_local_devices())\n","  gpus = tf.config.experimental.list_physical_devices('GPU')\n","  for gpu in gpus:\n","    try:\n","    \n","      tf.config.experimental.set_memory_growth(gpu, True)\n","      # Restrict TensorFlow to only use the first GPU\n","      tf.config.set_visible_devices(gpus[0], 'GPU')\n","      logical_gpus = tf.config.list_logical_devices('GPU')\n","      print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n","    except RuntimeError as e:\n","      # Visible devices must be set before GPUs have been initialized\n","      print(e)\n","\n","except Exception as error:\n","    print(\"Error trying to configure computing device.\")\n","    print(error)\n","\n","print(\"TensorFlow verison: \",tf.__version__)\n","print(\"CUDA verison: \", build.build_info['cuda_version'])\n","print(\"CUDNN verison: \", build.build_info['cudnn_version'])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m8bB9ucqnbka"},"source":["### Set Tensorflow-GPU precision \n","Mixed precision is the use of both 16-bit and 32-bit floating-point types in a model during training to make it run faster and use less memory. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lk8Y11Enbkc","outputId":"f1e40655-43f7-4a52-ee44-a2825e5da8a8"},"outputs":[{"data":{"text/plain":["'float32'"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# tf.keras.backend.floatx()\n","# tf.keras.backend.set_floatx('float16')\n","# tf.keras.backend.set_floatx('float32')\n","policy = tf.keras.mixed_precision.Policy('mixed_float16')\n","tf.keras.mixed_precision.set_global_policy(policy)\n","tf.keras.backend.floatx()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QUaBNmKC-cu"},"outputs":[],"source":["# Change as you wish\n","if IN_COLAB:\n","  TRAIN_FOLDER = './data/Train'\n","  VALIDATION_FOLDER = './data/Validation'\n","  IMAGE_INPUT_FOLDER = './data/Image_Input'\n","  IMAGE_OUTPUT_FOLDER = './data/Image_Output'\n","else:\n","  TRAIN_FOLDER = './../data/Train'\n","  VALIDATION_FOLDER = './../data/Validation'\n","  IMAGE_INPUT_FOLDER = './../data/Image_Input'\n","  IMAGE_OUTPUT_FOLDER = './../data/Image_Output'\n","\n","\n","# myData = pd.read_csv(os.path.join(DATASET_FOLDER, PENDULUM_DATA))\n","# myData.round(decimals=6)\n","# myData = myData.astype(np.float32)\n","# myData = myData.astype(np.float16)\n","# myData.describe().transpose()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBGuGJPOnbkd"},"outputs":[],"source":["# Set the paths to your annotated image folders\n","input_image_path = os.path.join(IMAGE_INPUT_FOLDER)\n","output_image_path = os.path.join(IMAGE_INPUT_FOLDER)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDsGXIkhC-cu"},"outputs":[],"source":["# Get the list of image and label file paths\n","input_filenames = glob.glob(os.path.join(input_image_path,'*.jpg'))\n","output_filenames = glob.glob(os.path.join(output_image_path,'*.jpg'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QAzrmcfUnbkd","outputId":"5cd8097a-b896-4b1d-fd20-4d29e09ce570"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["check_ims = [in_name.split('\\\\')[1]==out_name.split('\\\\')[1] for in_name,out_name in zip(input_filenames, output_filenames)]\n","any(not x for x in check_ims) #Check if at least one image names is not corresponding "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lm0A4DKtnbke"},"outputs":[],"source":["test_size = 0.1  #Specify test percentage\n","\n","# Split the image and label file paths into training and validation sets\n","train_image_paths, validation_image_paths, train_label_paths, validation_label_paths = train_test_split(input_filenames, output_filenames, test_size=0.2, shuffle=True)\n","\n","# Create the training and validation folders\n","train_folder_path = os.path.join(TRAIN_FOLDER)\n","validation_folder_path = os.path.join(VALIDATION_FOLDER)\n","\n","os.makedirs(os.path.join(train_folder_path, 'Input_Image'), exist_ok=True)\n","os.makedirs(os.path.join(train_folder_path, 'Output_Image'), exist_ok=True)\n","os.makedirs(os.path.join(validation_folder_path, 'Input_Image'), exist_ok=True)\n","os.makedirs(os.path.join(validation_folder_path, 'Output_Image'), exist_ok=True)\n","\n","# Copy the training images and labels to the training folder\n","for image_path, label_path in zip(train_image_paths, train_label_paths):\n","    image_name = os.path.basename(image_path)\n","    label_name = os.path.basename(label_path)\n","    shutil.copy(image_path, os.path.join(train_folder_path, 'Input_Image', image_name))\n","    shutil.copy(label_path, os.path.join(train_folder_path, 'Output_Image',label_name))\n","\n","# Copy the validation images and labels to the validation folder\n","for image_path, label_path in zip(validation_image_paths, validation_label_paths):\n","    image_name = os.path.basename(image_path)\n","    label_name = os.path.basename(label_path)\n","    shutil.copy(image_path, os.path.join(validation_folder_path, 'Input_Image', image_name))\n","    shutil.copy(label_path, os.path.join(validation_folder_path, 'Output_Image', label_name))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TozpCronnbkf"},"outputs":[],"source":["def resize_with_padding(image, target_size):\n","    \"\"\"\n","    Resizes an image with padding and maintains aspect ratio while keeping the background transparent.\n","    :param image: input image as a numpy array\n","    :param target_size: a tuple (width, height) representing the target size of the output image\n","    :return: the resized image as a numpy array\n","    \"\"\"\n","    h, w = image.shape[:2]\n","    target_w, target_h = target_size\n","\n","    # Calculate the scale factor and the new dimensions\n","    scale = min(target_w / w, target_h / h)\n","    new_w = int(w * scale)\n","    new_h = int(h * scale)\n","\n","    # Resize the image\n","    image_resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n","\n","    # Create a transparent background\n","    background = np.zeros((target_h, target_w, 4), dtype=np.uint8)\n","    background[:, :, 3] = 0  # set alpha channel to zero\n","\n","    # Calculate the coordinates to paste the resized image\n","    x = int((target_w - new_w) / 2)\n","    y = int((target_h - new_h) / 2)\n","\n","    # Paste the resized image onto the transparent background\n","    background[y:y+new_h, x:x+new_w, :3] = image_resized\n","    background[y:y+new_h, x:x+new_w, 3] = 255  # set alpha channel to 255 (fully opaque)\n","\n","    return background\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffrPUKG5nbkf"},"outputs":[],"source":["class CustomDataGenerator(Sequence):\n","    def __init__(self, input_paths, output_paths, batch_size, input_shape, num_classes=5, plot=False):\n","        self.input_paths = input_paths\n","        self.output_paths = output_paths\n","        self.batch_size = batch_size\n","        self.input_shape = input_shape\n","        self.plot = plot\n","        self.num_classes = num_classes\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.input_paths) / float(self.batch_size)))\n","\n","    def __getitem__(self, idx):\n","        input_batch_paths = self.input_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        output_batch_paths = self.output_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n","\n","        input_images = list(map(self.image_dataloader, input_batch_paths))\n","        output_images = list(map(self.image_dataloader, output_batch_paths))\n","\n","        return cupy.asnumpy(input_images)/255., cupy.asnumpy(output_images)/255.\n","        \n","\n","    def image_dataloader(self, image_path):\n","        # Load the input image\n","        image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = image.reshape(image.shape[0],image.shape[1],image.shape[2])\n","        # Resize the image with padding\n","        image = resize_with_padding(image, self.input_shape)\n","        # Add an alpha channel to the input image\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2RGBA)\n","        if self.plot:\n","            plt.imshow(image)\n","            plt.title('Image')\n","            plt.show()\n","        return image\n","        \n","    def resize_with_padding(self, image, target_size):\n","        \"\"\"\n","        Resizes an image with padding and maintains aspect ratio while keeping the background transparent.\n","        :param image: input image as a numpy array\n","        :param target_size: a tuple (width, height) representing the target size of the output image\n","        :return: the resized image as a numpy array\n","        \"\"\"\n","        h, w = image.shape[:2]\n","        target_w, target_h = target_size\n","\n","        # Calculate the scale factor and the new dimensions\n","        scale = min(target_w / w, target_h / h)\n","        new_w = int(w * scale)\n","        new_h = int(h * scale)\n","\n","        # Resize the image\n","        image_resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n","\n","        # Create a transparent background\n","        background = np.zeros((target_h, target_w, 4), dtype=np.uint8)\n","        background[:, :, 3] = 0  # set alpha channel to zero\n","\n","        # Calculate the coordinates to paste the resized image\n","        x = int((target_w - new_w) / 2)\n","        y = int((target_h - new_h) / 2)\n","\n","        # Paste the resized image onto the transparent background\n","        background[y:y+new_h, x:x+new_w, :3] = image_resized\n","        background[y:y+new_h, x:x+new_w, 3] = 255  # set alpha channel to 255 (fully opaque)\n","\n","        return background\n","\n","\n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHiyynqfnbkg"},"outputs":[],"source":["height, width = 100, 100\n","batch_size = 2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geV88M-snbkg","outputId":"6bb7a0d7-993e-4e81-dd97-6a7518b9e4ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape:  (2, 100, 100, 4) x_train dtype: float64\n","y_train shape:  (2, 100, 100, 4) y_train dtype: float64\n","x_val shape:  (2, 100, 100, 4) x_val dtype: float64\n","y_val shape:  (2, 100, 100, 4) y_val dtype: float64\n"]}],"source":["train_generator = CustomDataGenerator(train_image_paths, train_label_paths, batch_size, \n","                                      input_shape=(height, width), num_classes=5)\n","\n","validation_generator = CustomDataGenerator(validation_image_paths, validation_label_paths, batch_size, \n","                                           input_shape=(height, width), num_classes=5)\n","\n","input_train_batch, output_train_batch=next(iter(train_generator))\n","\n","print('x_train shape: ', input_train_batch.shape, 'x_train dtype:', input_train_batch.dtype)  \n","print('y_train shape: ', output_train_batch.shape, 'y_train dtype:', output_train_batch.dtype)\n","\n","input_validation_batch, output_validation_batch=next(iter(validation_generator))\n","\n","print('x_val shape: ', input_validation_batch.shape, 'x_val dtype:', input_validation_batch.dtype)  \n","print('y_val shape: ', output_validation_batch.shape, 'y_val dtype:', output_validation_batch.dtype)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubK3WlhZnbkg","outputId":"ce761e54-6975-4755-d0c7-1b749b1ced4b"},"outputs":[{"data":{"text/plain":["(2, 100, 100, 4)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["input_train_batch.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMsMfDfXnbkh","outputId":"98509f0b-4342-4246-ad43-61a68aff7b08"},"outputs":[{"ename":"ValueError","evalue":"A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 48, 48, 128), (None, 49, 49, 128)]","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[41], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m# concat7 = concatenate([up7, conv2])\u001b[39;00m\n\u001b[0;32m     37\u001b[0m crop_conv2 \u001b[39m=\u001b[39m Cropping2D(cropping\u001b[39m=\u001b[39m((\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), (\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)))(conv2)\n\u001b[1;32m---> 38\u001b[0m concat7 \u001b[39m=\u001b[39m concatenate([up7, crop_conv2])\n\u001b[0;32m     39\u001b[0m conv7 \u001b[39m=\u001b[39m Conv2D(\u001b[39m128\u001b[39m, \u001b[39m3\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(concat7)\n\u001b[0;32m     40\u001b[0m conv7 \u001b[39m=\u001b[39m Conv2D(\u001b[39m128\u001b[39m, \u001b[39m3\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(conv7)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\layers\\merging\\concatenate.py:231\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.layers.concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcatenate\u001b[39m(inputs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    201\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Functional interface to the `Concatenate` layer.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[0;32m    203\u001b[0m \u001b[39m    >>> x = np.arange(20).reshape(2, 2, 5)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39m        A tensor, the concatenation of the inputs alongside axis `axis`.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m Concatenate(axis\u001b[39m=\u001b[39;49maxis, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)(inputs)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\layers\\merging\\concatenate.py:131\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    125\u001b[0m unique_dims \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[0;32m    126\u001b[0m     shape[axis]\n\u001b[0;32m    127\u001b[0m     \u001b[39mfor\u001b[39;00m shape \u001b[39min\u001b[39;00m shape_set\n\u001b[0;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m shape[axis] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    129\u001b[0m )\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unique_dims) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err_msg)\n","\u001b[1;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 48, 48, 128), (None, 49, 49, 128)]"]}],"source":["\n","# Define the input shape of the image\n","input_shape = (height, width, 4)\n","\n","# Define the encoder layers\n","inputs = Input(input_shape)\n","conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n","conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n","pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","\n","conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n","conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n","pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n","conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n","pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","\n","conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n","conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n","drop4 = Dropout(0.5)(conv4)\n","pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","# Define the decoder layers with skip connections\n","up5 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(pool4)\n","concat5 = concatenate([up5, drop4])\n","conv5 = Conv2D(512, 3, activation='relu', padding='same')(concat5)\n","conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n","\n","up6 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5)\n","crop_conv3 = Cropping2D(cropping=((1, 0), (1, 0)))(conv3)\n","concat6 = concatenate([up6, crop_conv3])\n","conv6 = Conv2D(256, 3, activation='relu', padding='same')(concat6)\n","conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n","\n","up7 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)\n","# concat7 = concatenate([up7, conv2])\n","crop_conv2 = Cropping2D(cropping=((1, 0), (1, 0)))(conv2)\n","concat7 = concatenate([up7, crop_conv2])\n","conv7 = Conv2D(128, 3, activation='relu', padding='same')(concat7)\n","conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n","\n","up8 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)\n","concat8 = concatenate([up8, conv1])\n","crop_conv1 = Cropping2D(cropping=((1, 0), (1, 0)))(conv1)\n","concat8 = concatenate([up7, crop_conv1])\n","conv8 = Conv2D(64, 3, activation='relu', padding='same')(concat8)\n","conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n","\n","# Define the output layer with softmax activation\n","# output = Conv2D(num_classes, 1,  activation='softmax', padding='same')(conv8)\n","\n","# Define the model\n","model = Model(inputs=inputs, outputs=conv8)\n","\n","# Compile the model with appropriate loss and metrics\n","model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfCM4Pwenbkh"},"outputs":[],"source":["\n","# Load the annotated image\n","annotated_image = load_annotated_image('path/to/annotated/image.png')\n","\n","# Convert the annotation into categorical labels\n","categorical_labels = to_categorical(annotated_image, num_classes)\n","model.fit(x_train, y_train, batch_size=32, epochs=50, validation_data=(x_val, y_val))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"arnwIP-tnbki"},"outputs":[],"source":["# Train the model using the generators\n","model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=train_generator.samples // batch_size,\n","    epochs=50,\n","    validation_data=validation_generator,\n","    validation_steps=validation_generator.samples // batch_size)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZUOY62wnbki"},"outputs":[],"source":["\n","model.fit_generator(train_generator,\n","                    steps_per_epoch=len(train_image_paths) // batch_size,\n","                    epochs=epochs,\n","                    validation_data=validation_generator,\n","                    validation_steps=len(validation_image_paths) // batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2DsqKVknbki","outputId":"ec67a4ab-b4ca-49a1-b7d9-1fb61bde3d5c"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAEEAAAAfCAYAAAC4c6DCAAAABmJLR0QA/wD/AP+gvaeTAAADBklEQVRYhe2ZsUsjWxSHv2gI4ziJCSSFKDIpRBGDCilEm2QUBCFqoeZvsEhhsDOtFoLavNdEBAutbOwEbcRa0IhFQDQSUVNMRgmIaJH7mt1lNWImsEl2384HA3MOw70/vuLOGcYmhBD83fzbUO8EvwP27zc3Nzc8PDzUM0tNGRgYQJIk4CcJa2trNDU14Xa76xasVuzv77OxsUFXVxfwkwSAubk5VFWtR66aYhjGu9o6E7AkAJYEwJIAWBIASwJQgYSzszNaWlrY29urZh7TfJUnGo0SDAZNr2VaQn9/P/F43PTC1earPDs7OxWtVVZCPp9H0zQkSWJlZQWA2dlZRkZG6O7upq2tjbu7u097ZpiamsLpdCLLMs3NzXR0dOBwOAiHwwA8Pj4yOjqKoihomsbl5WVJHl3XGR4exuVysb6+XpEAUxKSySSBQIBCocD8/DwA8Xiczs5OLi4uCIVCpFKpT3tmSCQSRCIRDMNgZmaGpaUldF0nm83y9vZGMpkkGAySy+UIBAIEg8GSPJubm4yPj3N7e8vW1haVfhiXlXB1dcXExAQOhwO7/d2Ujd1ux+/3UywWv+yVQ5ZlJElCVVWcTiculwufz0exWCSTyRAKhVAUhbGxMQqFQkmebDZLIpHA7XZzfn5OPp83vTeYkNDa2srh4SHPz89cX19XtPivwO/3c3R0xMvLCwcHBzgcjpI87e3tLC8v8/r6ytPTE16vt7JNxDdisZjIZDLiI9lsVvT19Qmn0yl6enrE0NCQmJ6eFoqiiOPjY+HxeEQ4HP60Z4bJyUkhy7LY3d0VHo9HqKoqtre3RWNjo1hcXBSGYQhN04Qsy0LTNJFKpUry5HI5MTg4KGRZFrFYTESjUdHQ0CBWV1c/3XNhYUGk0+nv5T9lJfwf+SihKsOSruvYbDZTl67r1YhQEfbyj1SO1+ut+ISuJ9bYjCUBsCQAlgTAkgBYEoAPr8jT09O/4gfM/f39u/qHhEgkwsnJCel0uuahak1vby8+n+9HbRN/0lRTHawfsgD/AVGsimT/WNJTAAAAAElFTkSuQmCC","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_model(Sales_model, show_shapes=True, to_file=os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model.png'), show_layer_names=True, rankdir='TB', expand_nested=True, dpi=50)"]},{"cell_type":"markdown","metadata":{"id":"JjQOFXrZnbki"},"source":["## Implement Callback Functions\n","\n","During training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras.\n","This is the callback for writing checkpoints during training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfaBpa7znbkj"},"outputs":[],"source":["from tensorflow.python.framework.test_util import run_functions_eagerly\n","save_format = 'h5'\n","path_checkpoint = os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model.{save_format}')\n","callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n","                                      monitor='val_mse',\n","                                      verbose=1,\n","                                      save_weights_only=True,\n","                                      restore_best_weights=True,\n","                                      save_best_only=True, \n","                                      run_functions_eagerly=True,\n","                                      save_format=save_format)\n","\n","path_checkpoint_MA = os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model_MA.{save_format}')\n","\n","path_checkpoint_SWA = os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model_SWA.{save_format}')\n","\n","callback_MA = tfa.callbacks.AverageModelCheckpoint(filepath=path_checkpoint_MA, \n","                                                    monitor='val_mse',\n","                                                    update_weights=True,\n","                                                    save_weights_only=True,\n","                                                    run_functions_eagerly=True, \n","                                                    save_format=save_format)\n","\n","callback_SWA = tfa.callbacks.AverageModelCheckpoint(filepath=path_checkpoint_SWA,\n","                                                    monitor='val_mse',\n","                                                    update_weights=True,\n","                                                    save_weights_only=True,\n","                                                    run_functions_eagerly=True,\n","                                                    save_format=save_format)"]},{"cell_type":"markdown","metadata":{"id":"MT8g2IsFnbkk"},"source":["This is the callback for stopping the optimization when performance worsens on the validation-set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5Tv6qUDnbkk"},"outputs":[],"source":["callback_early_stopping = EarlyStopping(  monitor=\"val_mse\",\n","                                          # min_delta=0,\n","                                          patience=100,\n","                                          verbose=1,\n","                                          mode=\"auto\",\n","                                          # baseline=None,\n","                                          restore_best_weights=True )"]},{"cell_type":"markdown","metadata":{"id":"3TZYZOU3nbkl"},"source":["This is the callback for writing the TensorBoard log during training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SC1X6JHknbkl","outputId":"00bfd613-fc11-4021-9ab0-9a8176663d19"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: 'data/model/TensorBoard'\n","Directory '%s' can not be removed data/model/TensorBoard\n"]}],"source":["dirpaths = [Path(os.path.join(DATA_FOLDER, r'model//TensorBoard/'))]\n","\n","for dirpath in dirpaths:\n","    if dirpath.exists() and dirpath.is_dir():\n","        try:        \n","            shutil.rmtree(dirpath, ignore_errors=True)\n","            os.chmod(dirpath, 0o777)\n","            os.rmdir(dirpath)\n","            os.removedirs(dirpath)\n","            print(\"Directory '%s' has been removed successfully\", dirpath)\n","        except OSError as error:\n","            print(error)\n","            print(\"Directory '%s' can not be removed\", dirpath)\n","\n","logdir = os.path.join(DATA_FOLDER, r'model//TensorBoard/')\n","# logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","callback_tensorboard = TensorBoard(log_dir=logdir,\n","                                   histogram_freq=1,\n","                                   write_graph=True,\n","                                   profile_batch = '500,520')"]},{"cell_type":"markdown","metadata":{"id":"ExwTKXh3nbkl"},"source":["This callback reduces the learning-rate for the optimizer if the validation-loss has not improved since the last epoch (as indicated by `patience=100`). The learning-rate will be reduced by multiplying it with the given factor. We set a start learning-rate of 1e-2 above, so multiplying it by 0.95 gives a learning-rate of 9.5e-3. We don't want the learning-rate to go any lower 1e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCgfLMBLnbkl"},"outputs":[],"source":["callback_reduce_lr = ReduceLROnPlateau(monitor='val_mse',\n","                                       factor=0.99,\n","                                       min_lr=1e-5,\n","                                       patience=5,\n","                                       verbose=1)    \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhq9pxsmnbkl"},"outputs":[],"source":["from collections.abc import Iterable\n","from tensorflow.keras.callbacks import *\n","from tensorflow.python.ops import math_ops\n","from tensorflow.python.ops import control_flow_ops\n","from tensorflow.python.framework import constant_op\n","import math\n","\n","\n","class CosineDecayRestarts(tf.keras.callbacks.Callback):\n","    def __init__(self, initial_learning_rate, first_decay_steps, alpha=0.0, t_mul=2.0, m_mul=1.0):\n","        super(CosineDecayRestarts, self).__init__()\n","        self.initial_learning_rate = initial_learning_rate\n","        self.first_decay_steps = first_decay_steps\n","        self.alpha = alpha\n","        self.t_mul = t_mul\n","        self.m_mul = m_mul\n","        self.batch_step = 0\n","\n","    def on_train_batch_begin(self, step, logs=None):\n","        if not hasattr(self.model.optimizer, \"lr\"):\n","            raise ValueError('Optimizer must have a \"lr\" attribute.')\n","        # Get the current learning rate from model's optimizer.\n","        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n","        # Call schedule function to get the scheduled learning rate.\n","        scheduled_lr = self.schedule(self.batch_step, lr)\n","        # Set the value back to the optimizer before this epoch starts\n","        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n","        self.batch_step += 1\n","\n","    def schedule(self, step, lr):\n","        def compute_step(completed_fraction, geometric=False):\n","            \"\"\"Helper for `cond` operation.\"\"\"\n","            if geometric:\n","                i_restart = math_ops.floor(\n","                  math_ops.log(1.0 - completed_fraction * (1.0 - self.t_mul)) /\n","                  math_ops.log(self.t_mul))\n","\n","                sum_r = (1.0 - self.t_mul**i_restart) / (1.0 - self.t_mul)\n","                completed_fraction = (completed_fraction - sum_r) / self.t_mul**i_restart\n","\n","            else:\n","                i_restart = math_ops.floor(completed_fraction)\n","                completed_fraction -= i_restart\n","\n","            return i_restart, completed_fraction\n","\n","        completed_fraction = step / self.first_decay_steps\n","\n","        i_restart, completed_fraction = control_flow_ops.cond(\n","          math_ops.equal(self.t_mul, 1.0),\n","          lambda: compute_step(completed_fraction, geometric=False),\n","          lambda: compute_step(completed_fraction, geometric=True))\n","\n","        m_fac = self.m_mul**i_restart\n","        cosine_decayed = 0.5 * m_fac * (1.0 + math_ops.cos(\n","          constant_op.constant(math.pi) * completed_fraction))\n","        decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n","\n","        return math_ops.multiply(self.initial_learning_rate, decayed)\n","\n","callback_cosine_decay_restarts = CosineDecayRestarts( initial_learning_rate,\n","                                                      first_decay_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbwlBGZVnbkm"},"outputs":[],"source":["callbacks = [callback_early_stopping,\n","             callback_checkpoint,\n","             callback_tensorboard,\n","             callback_cosine_decay_restarts,\n","             callback_SWA,\n","             callback_reduce_lr]"]},{"cell_type":"markdown","metadata":{"id":"55KUnBUNnbkm"},"source":["#### Load weights from last checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8rq7zM4nbkm"},"outputs":[],"source":["def train_model(resume, epochs, initial_epoch, batch_size, model, savepath):\n","    with tf.device('/device:GPU:0'):\n","        print(model.summary())\n","        history=model.fit(x_train_generator, \n","                          steps_per_epoch=steps_per_epoch, \n","                          epochs=EPOCHS, \n","                          verbose=1, \n","                          callbacks=callbacks,\n","                          validation_data=x_val_generator, \n","                          validation_steps=train_validation_steps, \n","                          #validation_freq=5,\n","                          #class_weight=None, \n","                          #max_queue_size=10, \n","                          #workers=8, \n","                          #use_multiprocessing=True,\n","                          #shuffle=True, \n","                          initial_epoch=initial_epoch)\n","        \n","        model.load_weights(savepath)\n","        print(\"Checkpoint Loaded\")  \n","        model.evaluate(x_test_generator, steps=test_validation_steps)\n","\n","    if resume:\n","        try:\n","            #del model\n","            model.load_weights(savepath, {\"MeanExponentWeightedError\":MeanExponentWeightedError})\n","            #model = load_model(savepath, {\"MeanExponentWeightedError\":MeanExponentWeightedError})\n","            print(\"Checkpoint Loaded\")  \n","        except Exception as error:\n","            print(\"Error trying to load checkpoint.\")\n","            print(error)\n","\n","    return history, model\n","    \n","with tf.device('/device:GPU:0'):\n","    def plot_train_history(history, title):\n","        loss = history.history['loss']\n","        accuracy = history.history['acc']\n","        mape = history.history['mape']\n","        mae = history.history['mae']\n","        val_loss = history.history['val_loss']\n","        val_accuracy = history.history['val_acc']\n","        val_mae = history.history['val_mae']\n","        val_mape = history.history['val_mape']\n","        epochs = range(len(loss))\n","        plt.figure(figsize=(30,5))\n","        plt.plot(epochs, loss, label='training_loss') \n","        plt.plot(epochs, val_loss, label='validation_loss')\n","        plt.show()\n","        plt.figure(figsize=(30,5))\n","        plt.plot(epochs, accuracy, label='training_accuracy') \n","        plt.plot(epochs, val_accuracy, label='validation_accuracy')\n","        plt.show()\n","        plt.figure(figsize=(30,5))\n","        plt.plot(epochs, mae, label='training_mae') \n","        plt.plot(epochs, val_mae, label='validation_mae')\n","        plt.show()\n","        plt.figure(figsize=(30,5))\n","        plt.plot(epochs, mape, label='training_mape') \n","        plt.plot(epochs, val_mape, label='validation_mape')\n","        plt.show()\n","        return"]},{"cell_type":"markdown","metadata":{"id":"cxk35XWmnbkm"},"source":["## Train Model"]},{"cell_type":"markdown","metadata":{"id":"w3Yyq29znbkn"},"source":["# Load the TensorBoard notebook extension\n","# %load_ext tensorboard\n","%reload_ext tensorboard\n","\n","%tensorboard --logdir './data/model/TensorBoard/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bk8o-yGQnbkn","outputId":"167a41b1-32f8-4431-94b1-eb1137eac801"},"outputs":[{"name":"stdout","output_type":"stream","text":["Checkpoint Loaded\n","Model: \"dnn__model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," H1 (Dense)                  multiple                  10800     \n","                                                                 \n"," H2 (Dense)                  multiple                  300500    \n","                                                                 \n"," H3 (Dense)                  multiple                  200400    \n","                                                                 \n"," H4 (Dense)                  multiple                  120300    \n","                                                                 \n"," H5 (Dense)                  multiple                  60200     \n","                                                                 \n"," H6 (Dense)                  multiple                  20100     \n","                                                                 \n"," H7 (Dense)                  multiple                  1010      \n","                                                                 \n"," Tanh_Layer (Dense)          multiple                  1010      \n","                                                                 \n"," H8 (Dense)                  multiple                  2100      \n","                                                                 \n"," H9 (Dense)                  multiple                  40200     \n","                                                                 \n"," H10 (Dense)                 multiple                  60300     \n","                                                                 \n"," H11 (Dense)                 multiple                  240400    \n","                                                                 \n"," H12 (Dense)                 multiple                  400500    \n","                                                                 \n"," H13 (Dense)                 multiple                  600600    \n","                                                                 \n"," Silu_Layer (Dense)          multiple                  6005      \n","                                                                 \n"," Sigmoid_Layer (Dense)       multiple                  6005      \n","                                                                 \n"," Linear_Layer (Dense)        multiple                  1505      \n","                                                                 \n"," BN1 (BatchNormalization)    multiple                  2400      \n","                                                                 \n"," BN2 (BatchNormalization)    multiple                  2000      \n","                                                                 \n"," BN3 (BatchNormalization)    multiple                  1600      \n","                                                                 \n"," BN4 (BatchNormalization)    multiple                  1200      \n","                                                                 \n"," BN5 (BatchNormalization)    multiple                  800       \n","                                                                 \n"," BN6 (BatchNormalization)    multiple                  400       \n","                                                                 \n"," BN7 (BatchNormalization)    multiple                  80        \n","                                                                 \n"," BN8 (BatchNormalization)    multiple                  800       \n","                                                                 \n"," BN9 (BatchNormalization)    multiple                  800       \n","                                                                 \n"," BN10 (BatchNormalization)   multiple                  2400      \n","                                                                 \n"," BN11 (BatchNormalization)   multiple                  3200      \n","                                                                 \n"," BN12 (BatchNormalization)   multiple                  4000      \n","                                                                 \n"," BN13 (BatchNormalization)   multiple                  4800      \n","                                                                 \n"," D1 (Dropout)                multiple                  0         \n","                                                                 \n"," D2 (Dropout)                multiple                  0         \n","                                                                 \n"," D3 (Dropout)                multiple                  0         \n","                                                                 \n"," D4 (Dropout)                multiple                  0         \n","                                                                 \n"," D5 (Dropout)                multiple                  0         \n","                                                                 \n"," D6 (Dropout)                multiple                  0         \n","                                                                 \n"," D7 (Dropout)                multiple                  0         \n","                                                                 \n"," D8 (Dropout)                multiple                  0         \n","                                                                 \n"," D9 (Dropout)                multiple                  0         \n","                                                                 \n"," D10 (Dropout)               multiple                  0         \n","                                                                 \n"," D11 (Dropout)               multiple                  0         \n","                                                                 \n"," D12 (Dropout)               multiple                  0         \n","                                                                 \n"," D13 (Dropout)               multiple                  0         \n","                                                                 \n"," Flatten_Layer (Flatten)     multiple                  0         \n","                                                                 \n"," Output_Layer (Reshape)      multiple                  0         \n","                                                                 \n","=================================================================\n","Total params: 2,096,430\n","Trainable params: 2,084,190\n","Non-trainable params: 12,240\n","_________________________________________________________________\n","None\n","Epoch 1/10000\n","194/194 [==============================] - ETA: 0s - loss: 4953.7485 - mse: 4406.7866 - acc: 0.9325 - mae: 48.4201 - mape: 546.9600\n","Epoch 1: val_mse improved from inf to 7827028480.00000, saving model to ./data/model/DNN_Sales_model.h5\n","194/194 [==============================] - 66s 336ms/step - loss: 4953.7485 - mse: 4406.7866 - acc: 0.9325 - mae: 48.4201 - mape: 546.9600 - val_loss: 7827533824.0000 - val_mse: 7827028480.0000 - val_acc: 0.5800 - val_mae: 40702.8594 - val_mape: 505929.1875 - lr: 0.0094\n","Epoch 2/10000\n","194/194 [==============================] - ETA: 0s - loss: 5797.7119 - mse: 4983.0737 - acc: 0.9314 - mae: 50.1143 - mape: 814.6367\n","Epoch 2: val_mse did not improve from 7827028480.00000\n","194/194 [==============================] - 64s 328ms/step - loss: 5797.7119 - mse: 4983.0737 - acc: 0.9314 - mae: 50.1143 - mape: 814.6367 - val_loss: 2114249056845824.0000 - val_mse: 2114248788410368.0000 - val_acc: 0.5800 - val_mae: 13108014.0000 - val_mape: 133572072.0000 - lr: 0.0076\n","Epoch 3/10000\n","194/194 [==============================] - ETA: 0s - loss: 5543.9746 - mse: 4783.7998 - acc: 0.9165 - mae: 49.7890 - mape: 760.1783\n","Epoch 3: val_mse improved from 7827028480.00000 to 5812.86768, saving model to ./data/model/DNN_Sales_model.h5\n","194/194 [==============================] - 76s 394ms/step - loss: 5543.9746 - mse: 4783.7998 - acc: 0.9165 - mae: 49.7890 - mape: 760.1783 - val_loss: 6392.6094 - val_mse: 5812.8677 - val_acc: 0.5600 - val_mae: 59.2190 - val_mape: 579.7420 - lr: 0.0052\n","Epoch 4/10000\n","194/194 [==============================] - ETA: 0s - loss: 5037.3188 - mse: 4476.1421 - acc: 0.9284 - mae: 48.1029 - mape: 561.1779\n","Epoch 4: val_mse improved from 5812.86768 to 5380.38135, saving model to ./data/model/DNN_Sales_model.h5\n","194/194 [==============================] - 66s 341ms/step - loss: 5037.3188 - mse: 4476.1421 - acc: 0.9284 - mae: 48.1029 - mape: 561.1779 - val_loss: 6180.4839 - val_mse: 5380.3813 - val_acc: 0.5700 - val_mae: 54.8648 - val_mape: 800.1016 - lr: 0.0028\n","Epoch 5/10000\n","194/194 [==============================] - ETA: 0s - loss: 6028.8799 - mse: 5069.8789 - acc: 0.9201 - mae: 49.5044 - mape: 959.0013\n","Epoch 5: val_mse improved from 5380.38135 to 4864.55225, saving model to ./data/model/DNN_Sales_model.h5\n","194/194 [==============================] - 66s 341ms/step - loss: 6028.8799 - mse: 5069.8789 - acc: 0.9201 - mae: 49.5044 - mape: 959.0013 - val_loss: 6078.9443 - val_mse: 4864.5522 - val_acc: 0.5400 - val_mae: 50.7770 - val_mape: 1214.3929 - lr: 8.8680e-04\n","Epoch 6/10000\n","194/194 [==============================] - ETA: 0s - loss: 5335.9443 - mse: 4593.2983 - acc: 0.9186 - mae: 47.9896 - mape: 742.6458\n","Epoch 6: val_mse did not improve from 4864.55225\n","194/194 [==============================] - 67s 343ms/step - loss: 5335.9443 - mse: 4593.2983 - acc: 0.9186 - mae: 47.9896 - mape: 742.6458 - val_loss: 8414.7227 - val_mse: 6011.9995 - val_acc: 0.5600 - val_mae: 55.6189 - val_mape: 2402.7236 - lr: 2.3439e-05\n","Epoch 7/10000\n","194/194 [==============================] - ETA: 0s - loss: 5421.4927 - mse: 4698.2163 - acc: 0.9294 - mae: 49.8744 - mape: 723.2780\n","Epoch 7: val_mse did not improve from 4864.55225\n","194/194 [==============================] - 67s 344ms/step - loss: 5421.4927 - mse: 4698.2163 - acc: 0.9294 - mae: 49.8744 - mape: 723.2780 - val_loss: 620680292474028032.0000 - val_mse: 620680292474028032.0000 - val_acc: 0.5300 - val_mae: 170121456.0000 - val_mape: 740779968.0000 - lr: 0.0099\n","Epoch 8/10000\n","194/194 [==============================] - ETA: 0s - loss: 5391.2954 - mse: 4760.6958 - acc: 0.9361 - mae: 49.7826 - mape: 630.6003\n","Epoch 8: val_mse did not improve from 4864.55225\n","194/194 [==============================] - 67s 346ms/step - loss: 5391.2954 - mse: 4760.6958 - acc: 0.9361 - mae: 49.7826 - mape: 630.6003 - val_loss: 7404.4136 - val_mse: 5722.6880 - val_acc: 0.5800 - val_mae: 54.9874 - val_mape: 1681.7253 - lr: 0.0095\n","Epoch 9/10000\n","194/194 [==============================] - ETA: 0s - loss: 5311.9932 - mse: 4666.1904 - acc: 0.9232 - mae: 48.1458 - mape: 645.8030\n","Epoch 9: val_mse did not improve from 4864.55225\n","194/194 [==============================] - 67s 345ms/step - loss: 5311.9932 - mse: 4666.1904 - acc: 0.9232 - mae: 48.1458 - mape: 645.8030 - val_loss: 7644.1138 - val_mse: 6319.5352 - val_acc: 0.6400 - val_mae: 58.9124 - val_mape: 1324.5786 - lr: 0.0088\n","Epoch 10/10000\n","194/194 [==============================] - ETA: 0s - loss: 5293.4980 - mse: 4548.9209 - acc: 0.9155 - mae: 48.5413 - mape: 744.5790\n","Epoch 10: val_mse did not improve from 4864.55225\n","\n","Epoch 10: ReduceLROnPlateau reducing learning rate to 0.007759048892185092.\n","194/194 [==============================] - 67s 345ms/step - loss: 5293.4980 - mse: 4548.9209 - acc: 0.9155 - mae: 48.5413 - mape: 744.5790 - val_loss: 10724.3223 - val_mse: 10069.5840 - val_acc: 0.6500 - val_mae: 78.4216 - val_mape: 654.7380 - lr: 0.0078\n","Epoch 11/10000\n","194/194 [==============================] - ETA: 0s - loss: 5318.3555 - mse: 4645.1396 - acc: 0.9320 - mae: 48.8329 - mape: 673.2127\n","Epoch 11: val_mse did not improve from 4864.55225\n","194/194 [==============================] - 67s 346ms/step - loss: 5318.3555 - mse: 4645.1396 - acc: 0.9320 - mae: 48.8329 - mape: 673.2127 - val_loss: 5922.6113 - val_mse: 5790.3198 - val_acc: 0.6000 - val_mae: 58.7343 - val_mape: 132.2915 - lr: 0.0067\n","Epoch 12/10000\n","194/194 [==============================] - ETA: 0s - loss: 5132.9692 - mse: 4504.3774 - acc: 0.9268 - mae: 48.1513 - mape: 628.5934\n","Epoch 12: val_mse did not improve from 4864.55225\n","194/194 [==============================] - 67s 348ms/step - loss: 5132.9692 - mse: 4504.3774 - acc: 0.9268 - mae: 48.1513 - mape: 628.5934 - val_loss: 9243.9180 - val_mse: 7468.6670 - val_acc: 0.6000 - val_mae: 65.2507 - val_mape: 1775.2506 - lr: 0.0055\n","Epoch 13/10000\n","194/194 [==============================] - ETA: 0s - loss: 5356.5752 - mse: 4631.5889 - acc: 0.9299 - mae: 48.2782 - mape: 724.9841\n","Epoch 13: val_mse did not improve from 4864.55225\n","194/194 [==============================] - 67s 347ms/step - loss: 5356.5752 - mse: 4631.5889 - acc: 0.9299 - mae: 48.2782 - mape: 724.9841 - val_loss: 8637.4541 - val_mse: 7538.3276 - val_acc: 0.6600 - val_mae: 67.9082 - val_mape: 1099.1256 - lr: 0.0042\n","Epoch 14/10000\n","194/194 [==============================] - ETA: 0s - loss: 5431.0063 - mse: 4653.9043 - acc: 0.9253 - mae: 47.7060 - mape: 777.1014\n","Epoch 14: val_mse did not improve from 4864.55225\n","194/194 [==============================] - 67s 343ms/step - loss: 5431.0063 - mse: 4653.9043 - acc: 0.9253 - mae: 47.7060 - mape: 777.1014 - val_loss: 7630.0444 - val_mse: 6936.5840 - val_acc: 0.6000 - val_mae: 63.9425 - val_mape: 693.4603 - lr: 0.0030\n","Epoch 15/10000\n","194/194 [==============================] - ETA: 0s - loss: 4967.4741 - mse: 4365.8647 - acc: 0.9304 - mae: 47.4383 - mape: 601.6078\n","Epoch 15: val_mse improved from 4864.55225 to 4278.43994, saving model to ./data/model/DNN_Sales_model.h5\n","194/194 [==============================] - 67s 348ms/step - loss: 4967.4741 - mse: 4365.8647 - acc: 0.9304 - mae: 47.4383 - mape: 601.6078 - val_loss: 5274.2144 - val_mse: 4278.4399 - val_acc: 0.6300 - val_mae: 48.6834 - val_mape: 995.7745 - lr: 0.0019\n","Epoch 16/10000\n","194/194 [==============================] - ETA: 0s - loss: 5300.8604 - mse: 4564.0908 - acc: 0.9309 - mae: 47.9217 - mape: 736.7676\n","Epoch 16: val_mse did not improve from 4278.43994\n","194/194 [==============================] - 67s 344ms/step - loss: 5300.8604 - mse: 4564.0908 - acc: 0.9309 - mae: 47.9217 - mape: 736.7676 - val_loss: 6316.2949 - val_mse: 4659.5879 - val_acc: 0.5900 - val_mae: 49.7266 - val_mape: 1656.7069 - lr: 0.0010\n","Epoch 17/10000\n","194/194 [==============================] - ETA: 0s - loss: 5049.5005 - mse: 4402.9907 - acc: 0.9216 - mae: 46.7871 - mape: 646.5092\n","Epoch 17: val_mse did not improve from 4278.43994\n","194/194 [==============================] - 67s 347ms/step - loss: 5049.5005 - mse: 4402.9907 - acc: 0.9216 - mae: 46.7871 - mape: 646.5092 - val_loss: 5734.5332 - val_mse: 4427.6289 - val_acc: 0.6500 - val_mae: 49.4330 - val_mape: 1306.9045 - lr: 3.8815e-04\n","Epoch 18/10000\n","194/194 [==============================] - ETA: 0s - loss: 5018.4580 - mse: 4427.2783 - acc: 0.9304 - mae: 47.7826 - mape: 591.1793\n","Epoch 18: val_mse improved from 4278.43994 to 3911.38623, saving model to ./data/model/DNN_Sales_model.h5\n","194/194 [==============================] - 69s 355ms/step - loss: 5018.4580 - mse: 4427.2783 - acc: 0.9304 - mae: 47.7826 - mape: 591.1793 - val_loss: 4461.9668 - val_mse: 3911.3862 - val_acc: 0.6400 - val_mae: 47.7239 - val_mape: 550.5807 - lr: 5.0808e-05\n","Epoch 19/10000\n","194/194 [==============================] - ETA: 0s - loss: 5425.5796 - mse: 4757.9707 - acc: 0.9325 - mae: 49.1322 - mape: 667.6057\n","Epoch 19: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 67s 348ms/step - loss: 5425.5796 - mse: 4757.9707 - acc: 0.9325 - mae: 49.1322 - mape: 667.6057 - val_loss: 1308093184.0000 - val_mse: 1307710720.0000 - val_acc: 0.4900 - val_mae: 11944.3936 - val_mape: 382419.0000 - lr: 0.0100\n","Epoch 20/10000\n","194/194 [==============================] - ETA: 0s - loss: 5650.8535 - mse: 4841.9790 - acc: 0.9222 - mae: 49.7870 - mape: 808.8774\n","Epoch 20: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 68s 350ms/step - loss: 5650.8535 - mse: 4841.9790 - acc: 0.9222 - mae: 49.7870 - mape: 808.8774 - val_loss: 4441.9546 - val_mse: 4068.6570 - val_acc: 0.7200 - val_mae: 49.8875 - val_mape: 373.2975 - lr: 0.0099\n","Epoch 21/10000\n","194/194 [==============================] - ETA: 0s - loss: 5158.7612 - mse: 4530.0220 - acc: 0.9232 - mae: 48.2545 - mape: 628.7380\n","Epoch 21: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 68s 351ms/step - loss: 5158.7612 - mse: 4530.0220 - acc: 0.9232 - mae: 48.2545 - mape: 628.7380 - val_loss: 7971.0059 - val_mse: 7548.7798 - val_acc: 0.6800 - val_mae: 65.1515 - val_mape: 422.2256 - lr: 0.0098\n","Epoch 22/10000\n","194/194 [==============================] - ETA: 0s - loss: 5807.7358 - mse: 5056.5898 - acc: 0.9268 - mae: 50.8687 - mape: 751.1451\n","Epoch 22: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 68s 353ms/step - loss: 5807.7358 - mse: 5056.5898 - acc: 0.9268 - mae: 50.8687 - mape: 751.1451 - val_loss: 7722.6338 - val_mse: 7052.2969 - val_acc: 0.6600 - val_mae: 63.5735 - val_mape: 670.3376 - lr: 0.0095\n","Epoch 23/10000\n","194/194 [==============================] - ETA: 0s - loss: 5440.9072 - mse: 4793.2593 - acc: 0.9268 - mae: 49.9183 - mape: 647.6484\n","Epoch 23: val_mse did not improve from 3911.38623\n","\n","Epoch 23: ReduceLROnPlateau reducing learning rate to 0.009134623222053051.\n","194/194 [==============================] - 69s 354ms/step - loss: 5440.9072 - mse: 4793.2593 - acc: 0.9268 - mae: 49.9183 - mape: 647.6484 - val_loss: 25385288.0000 - val_mse: 25371200.0000 - val_acc: 0.5100 - val_mae: 1658.5117 - val_mape: 14087.6934 - lr: 0.0092\n","Epoch 24/10000\n","194/194 [==============================] - ETA: 0s - loss: 5278.0645 - mse: 4571.8481 - acc: 0.9345 - mae: 48.5672 - mape: 706.2183\n","Epoch 24: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 68s 352ms/step - loss: 5278.0645 - mse: 4571.8481 - acc: 0.9345 - mae: 48.5672 - mape: 706.2183 - val_loss: 224684146688.0000 - val_mse: 224680771584.0000 - val_acc: 0.2000 - val_mae: 135862.2344 - val_mape: 3363003.5000 - lr: 0.0089\n","Epoch 25/10000\n","194/194 [==============================] - ETA: 0s - loss: 4904.7842 - mse: 4319.2290 - acc: 0.9294 - mae: 47.8064 - mape: 585.5543\n","Epoch 25: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 77s 400ms/step - loss: 4904.7842 - mse: 4319.2290 - acc: 0.9294 - mae: 47.8064 - mape: 585.5543 - val_loss: 5577.0610 - val_mse: 4919.5298 - val_acc: 0.5700 - val_mae: 55.4844 - val_mape: 657.5316 - lr: 0.0084\n","Epoch 26/10000\n","194/194 [==============================] - ETA: 0s - loss: 4953.3125 - mse: 4437.9365 - acc: 0.9340 - mae: 47.9834 - mape: 515.3752\n","Epoch 26: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 69s 355ms/step - loss: 4953.3125 - mse: 4437.9365 - acc: 0.9340 - mae: 47.9834 - mape: 515.3752 - val_loss: 6097.9058 - val_mse: 5287.6792 - val_acc: 0.6900 - val_mae: 56.6144 - val_mape: 810.2256 - lr: 0.0079\n","Epoch 27/10000\n","194/194 [==============================] - ETA: 0s - loss: 5087.5527 - mse: 4457.5610 - acc: 0.9299 - mae: 48.1585 - mape: 629.9924\n","Epoch 27: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 69s 355ms/step - loss: 5087.5527 - mse: 4457.5610 - acc: 0.9299 - mae: 48.1585 - mape: 629.9924 - val_loss: 18731.3184 - val_mse: 15991.7510 - val_acc: 0.6600 - val_mae: 77.5570 - val_mape: 2739.5647 - lr: 0.0074\n","Epoch 28/10000\n","194/194 [==============================] - ETA: 0s - loss: 5167.7388 - mse: 4562.8477 - acc: 0.9309 - mae: 48.6841 - mape: 604.8906\n","Epoch 28: val_mse did not improve from 3911.38623\n","\n","Epoch 28: ReduceLROnPlateau reducing learning rate to 0.006751112421043217.\n","194/194 [==============================] - 69s 357ms/step - loss: 5167.7388 - mse: 4562.8477 - acc: 0.9309 - mae: 48.6841 - mape: 604.8906 - val_loss: 7502.8057 - val_mse: 6534.3481 - val_acc: 0.6400 - val_mae: 62.0281 - val_mape: 968.4579 - lr: 0.0068\n","Epoch 29/10000\n","194/194 [==============================] - ETA: 0s - loss: 5429.4663 - mse: 4667.5459 - acc: 0.9206 - mae: 48.1570 - mape: 761.9218\n","Epoch 29: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 69s 356ms/step - loss: 5429.4663 - mse: 4667.5459 - acc: 0.9206 - mae: 48.1570 - mape: 761.9218 - val_loss: 7090.5381 - val_mse: 6927.8687 - val_acc: 0.6200 - val_mae: 71.7414 - val_mape: 162.6691 - lr: 0.0062\n","Epoch 30/10000\n","194/194 [==============================] - ETA: 0s - loss: 5473.3804 - mse: 4622.4370 - acc: 0.9196 - mae: 48.7406 - mape: 850.9421\n","Epoch 30: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 69s 356ms/step - loss: 5473.3804 - mse: 4622.4370 - acc: 0.9196 - mae: 48.7406 - mape: 850.9421 - val_loss: 6624.6138 - val_mse: 6215.0552 - val_acc: 0.6600 - val_mae: 60.0953 - val_mape: 409.5588 - lr: 0.0056\n","Epoch 31/10000\n","194/194 [==============================] - ETA: 0s - loss: 4880.5190 - mse: 4368.3716 - acc: 0.9325 - mae: 48.0390 - mape: 512.1459\n","Epoch 31: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 69s 358ms/step - loss: 4880.5190 - mse: 4368.3716 - acc: 0.9325 - mae: 48.0390 - mape: 512.1459 - val_loss: 6981.2705 - val_mse: 5592.8481 - val_acc: 0.5900 - val_mae: 53.6118 - val_mape: 1388.4222 - lr: 0.0050\n","Epoch 32/10000\n","194/194 [==============================] - ETA: 0s - loss: 5333.5010 - mse: 4605.0527 - acc: 0.9237 - mae: 47.9182 - mape: 728.4498\n","Epoch 32: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 69s 356ms/step - loss: 5333.5010 - mse: 4605.0527 - acc: 0.9237 - mae: 47.9182 - mape: 728.4498 - val_loss: 9991.4336 - val_mse: 9247.2227 - val_acc: 0.3100 - val_mae: 77.0431 - val_mape: 744.2114 - lr: 0.0043\n","Epoch 33/10000\n","194/194 [==============================] - ETA: 0s - loss: 4836.4102 - mse: 4321.4751 - acc: 0.9402 - mae: 47.5218 - mape: 514.9343\n","Epoch 33: val_mse did not improve from 3911.38623\n","\n","Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0036657167016528546.\n","194/194 [==============================] - 70s 361ms/step - loss: 4836.4102 - mse: 4321.4751 - acc: 0.9402 - mae: 47.5218 - mape: 514.9343 - val_loss: 7608.8267 - val_mse: 6192.2876 - val_acc: 0.6200 - val_mae: 60.5714 - val_mape: 1416.5391 - lr: 0.0037\n","Epoch 34/10000\n","194/194 [==============================] - ETA: 0s - loss: 4761.4580 - mse: 4206.6689 - acc: 0.9242 - mae: 46.4600 - mape: 554.7877\n","Epoch 34: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 69s 355ms/step - loss: 4761.4580 - mse: 4206.6689 - acc: 0.9242 - mae: 46.4600 - mape: 554.7877 - val_loss: 5144.1982 - val_mse: 4143.0498 - val_acc: 0.6000 - val_mae: 48.8666 - val_mape: 1001.1481 - lr: 0.0031\n","Epoch 35/10000\n","194/194 [==============================] - ETA: 0s - loss: 5322.2812 - mse: 4606.9604 - acc: 0.9211 - mae: 48.0888 - mape: 715.3228\n","Epoch 35: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 69s 356ms/step - loss: 5322.2812 - mse: 4606.9604 - acc: 0.9211 - mae: 48.0888 - mape: 715.3228 - val_loss: 5748.4312 - val_mse: 4557.7671 - val_acc: 0.5700 - val_mae: 50.3327 - val_mape: 1190.6642 - lr: 0.0025\n","Epoch 36/10000\n","194/194 [==============================] - ETA: 0s - loss: 5270.3540 - mse: 4552.8887 - acc: 0.9278 - mae: 48.2529 - mape: 717.4653\n","Epoch 36: val_mse did not improve from 3911.38623\n","194/194 [==============================] - 69s 356ms/step - loss: 5270.3540 - mse: 4552.8887 - acc: 0.9278 - mae: 48.2529 - mape: 717.4653 - val_loss: 5637.9224 - val_mse: 4330.7075 - val_acc: 0.6300 - val_mae: 48.6554 - val_mape: 1307.2156 - lr: 0.0020\n","Epoch 37/10000\n","194/194 [==============================] - ETA: 0s - loss: 4922.0659 - mse: 4339.4883 - acc: 0.9247 - mae: 47.2908 - mape: 582.5789\n","Epoch 37: val_mse improved from 3911.38623 to 3724.51880, saving model to ./data/model/DNN_Sales_model.h5\n","194/194 [==============================] - 70s 360ms/step - loss: 4922.0659 - mse: 4339.4883 - acc: 0.9247 - mae: 47.2908 - mape: 582.5789 - val_loss: 4268.6631 - val_mse: 3724.5188 - val_acc: 0.5900 - val_mae: 47.5589 - val_mape: 544.1443 - lr: 0.0015\n","Epoch 38/10000\n","194/194 [==============================] - ETA: 0s - loss: 5030.0537 - mse: 4409.0957 - acc: 0.9278 - mae: 46.9579 - mape: 620.9565\n","Epoch 38: val_mse did not improve from 3724.51880\n","194/194 [==============================] - 69s 357ms/step - loss: 5030.0537 - mse: 4409.0957 - acc: 0.9278 - mae: 46.9579 - mape: 620.9565 - val_loss: 5365.4731 - val_mse: 4426.7314 - val_acc: 0.6500 - val_mae: 50.0106 - val_mape: 938.7413 - lr: 0.0011\n","Epoch 39/10000\n","194/194 [==============================] - ETA: 0s - loss: 5400.8560 - mse: 4623.2930 - acc: 0.9268 - mae: 48.1075 - mape: 777.5628\n","Epoch 39: val_mse did not improve from 3724.51880\n","194/194 [==============================] - 70s 359ms/step - loss: 5400.8560 - mse: 4623.2930 - acc: 0.9268 - mae: 48.1075 - mape: 777.5628 - val_loss: 6083.0698 - val_mse: 5062.5005 - val_acc: 0.6500 - val_mae: 53.4550 - val_mape: 1020.5695 - lr: 7.2827e-04\n","Epoch 40/10000\n","194/194 [==============================] - ETA: 0s - loss: 5134.8159 - mse: 4442.8271 - acc: 0.9253 - mae: 47.3063 - mape: 691.9896\n","Epoch 40: val_mse did not improve from 3724.51880\n","194/194 [==============================] - 72s 368ms/step - loss: 5134.8159 - mse: 4442.8271 - acc: 0.9253 - mae: 47.3063 - mape: 691.9896 - val_loss: 6583.7876 - val_mse: 5086.7437 - val_acc: 0.5900 - val_mae: 52.1530 - val_mape: 1497.0437 - lr: 4.3361e-04\n","Epoch 41/10000\n","194/194 [==============================] - ETA: 0s - loss: 4707.2939 - mse: 4187.1582 - acc: 0.9309 - mae: 46.6088 - mape: 520.1354\n","Epoch 41: val_mse improved from 3724.51880 to 3723.71118, saving model to ./data/model/DNN_Sales_model.h5\n","194/194 [==============================] - 70s 362ms/step - loss: 4707.2939 - mse: 4187.1582 - acc: 0.9309 - mae: 46.6088 - mape: 520.1354 - val_loss: 4540.5039 - val_mse: 3723.7112 - val_acc: 0.6100 - val_mae: 45.8378 - val_mape: 816.7928 - lr: 2.1246e-04\n","Epoch 42/10000\n","194/194 [==============================] - ETA: 0s - loss: 4579.0303 - mse: 4143.2104 - acc: 0.9309 - mae: 46.6437 - mape: 435.8209\n","Epoch 42: val_mse did not improve from 3723.71118\n","194/194 [==============================] - 70s 361ms/step - loss: 4579.0303 - mse: 4143.2104 - acc: 0.9309 - mae: 46.6437 - mape: 435.8209 - val_loss: 6819.4111 - val_mse: 4947.6479 - val_acc: 0.6300 - val_mae: 50.8055 - val_mape: 1871.7642 - lr: 6.8392e-05\n","Epoch 43/10000\n","194/194 [==============================] - ETA: 0s - loss: 4925.3740 - mse: 4335.9189 - acc: 0.9351 - mae: 46.7252 - mape: 589.4562\n","Epoch 43: val_mse improved from 3723.71118 to 3490.17822, saving model to ./data/model/DNN_Sales_model.h5\n","194/194 [==============================] - 71s 364ms/step - loss: 4925.3740 - mse: 4335.9189 - acc: 0.9351 - mae: 46.7252 - mape: 589.4562 - val_loss: 3801.8994 - val_mse: 3490.1782 - val_acc: 0.6300 - val_mae: 44.6145 - val_mape: 311.7212 - lr: 3.7274e-06\n","Epoch 44/10000\n","194/194 [==============================] - ETA: 0s - loss: 5877.1519 - mse: 5049.2896 - acc: 0.9149 - mae: 50.7513 - mape: 827.8653\n","Epoch 44: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 72s 374ms/step - loss: 5877.1519 - mse: 5049.2896 - acc: 0.9149 - mae: 50.7513 - mape: 827.8653 - val_loss: 49882.3164 - val_mse: 49598.3750 - val_acc: 0.2800 - val_mae: 175.8915 - val_mape: 283.9445 - lr: 0.0100\n","Epoch 45/10000\n","194/194 [==============================] - ETA: 0s - loss: 5279.4297 - mse: 4594.5562 - acc: 0.9227 - mae: 48.3441 - mape: 684.8730\n","Epoch 45: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 71s 364ms/step - loss: 5279.4297 - mse: 4594.5562 - acc: 0.9227 - mae: 48.3441 - mape: 684.8730 - val_loss: 10349415.0000 - val_mse: 10341991.0000 - val_acc: 0.4000 - val_mae: 2016.5275 - val_mape: 7425.2451 - lr: 0.0100\n","Epoch 46/10000\n","194/194 [==============================] - ETA: 0s - loss: 4804.9668 - mse: 4231.4980 - acc: 0.9247 - mae: 47.3038 - mape: 573.4679\n","Epoch 46: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 71s 364ms/step - loss: 4804.9668 - mse: 4231.4980 - acc: 0.9247 - mae: 47.3038 - mape: 573.4679 - val_loss: 1342443224374968320.0000 - val_mse: 1342443224374968320.0000 - val_acc: 0.5900 - val_mae: 262339008.0000 - val_mape: 121919000.0000 - lr: 0.0099\n","Epoch 47/10000\n","194/194 [==============================] - ETA: 0s - loss: 5689.8384 - mse: 4931.9087 - acc: 0.9211 - mae: 50.2449 - mape: 757.9308\n","Epoch 47: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 71s 364ms/step - loss: 5689.8384 - mse: 4931.9087 - acc: 0.9211 - mae: 50.2449 - mape: 757.9308 - val_loss: 7163.6191 - val_mse: 6440.1348 - val_acc: 0.5800 - val_mae: 61.8166 - val_mape: 723.4842 - lr: 0.0099\n","Epoch 48/10000\n","194/194 [==============================] - ETA: 0s - loss: 5650.2690 - mse: 4885.7993 - acc: 0.9397 - mae: 50.0870 - mape: 764.4680\n","Epoch 48: val_mse did not improve from 3490.17822\n","\n","Epoch 48: ReduceLROnPlateau reducing learning rate to 0.00968165136873722.\n","194/194 [==============================] - 73s 376ms/step - loss: 5650.2690 - mse: 4885.7993 - acc: 0.9397 - mae: 50.0870 - mape: 764.4680 - val_loss: 192219045129355264.0000 - val_mse: 192219045129355264.0000 - val_acc: 0.4900 - val_mae: 177091456.0000 - val_mape: 694348096.0000 - lr: 0.0098\n","Epoch 49/10000\n","194/194 [==============================] - ETA: 0s - loss: 5244.5068 - mse: 4664.1191 - acc: 0.9299 - mae: 48.7594 - mape: 580.3886\n","Epoch 49: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 72s 369ms/step - loss: 5244.5068 - mse: 4664.1191 - acc: 0.9299 - mae: 48.7594 - mape: 580.3886 - val_loss: 8341.1475 - val_mse: 6996.6567 - val_acc: 0.7200 - val_mae: 64.2757 - val_mape: 1344.4916 - lr: 0.0097\n","Epoch 50/10000\n","194/194 [==============================] - ETA: 0s - loss: 5165.7383 - mse: 4520.3521 - acc: 0.9222 - mae: 48.7903 - mape: 645.3873\n","Epoch 50: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 71s 365ms/step - loss: 5165.7383 - mse: 4520.3521 - acc: 0.9222 - mae: 48.7903 - mape: 645.3873 - val_loss: 5281.3535 - val_mse: 4564.2109 - val_acc: 0.7000 - val_mae: 51.1431 - val_mape: 717.1429 - lr: 0.0096\n","Epoch 51/10000\n","194/194 [==============================] - ETA: 0s - loss: 5078.6094 - mse: 4488.5591 - acc: 0.9314 - mae: 48.2695 - mape: 590.0488\n","Epoch 51: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 73s 375ms/step - loss: 5078.6094 - mse: 4488.5591 - acc: 0.9314 - mae: 48.2695 - mape: 590.0488 - val_loss: 3693.8950 - val_mse: 3656.2173 - val_acc: 0.6300 - val_mae: 48.6982 - val_mape: 37.6779 - lr: 0.0094\n","Epoch 52/10000\n","194/194 [==============================] - ETA: 0s - loss: 5201.5327 - mse: 4558.6187 - acc: 0.9366 - mae: 48.9612 - mape: 642.9142\n","Epoch 52: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 71s 368ms/step - loss: 5201.5327 - mse: 4558.6187 - acc: 0.9366 - mae: 48.9612 - mape: 642.9142 - val_loss: 7157.8306 - val_mse: 6466.4414 - val_acc: 0.6200 - val_mae: 60.6062 - val_mape: 691.3901 - lr: 0.0093\n","Epoch 53/10000\n","194/194 [==============================] - ETA: 0s - loss: 5580.8560 - mse: 4822.3892 - acc: 0.9299 - mae: 49.4549 - mape: 758.4686\n","Epoch 53: val_mse did not improve from 3490.17822\n","\n","Epoch 53: ReduceLROnPlateau reducing learning rate to 0.008991440702229738.\n","194/194 [==============================] - 72s 373ms/step - loss: 5580.8560 - mse: 4822.3892 - acc: 0.9299 - mae: 49.4549 - mape: 758.4686 - val_loss: 5401.8818 - val_mse: 4661.1401 - val_acc: 0.5900 - val_mae: 52.2976 - val_mape: 740.7419 - lr: 0.0091\n","Epoch 54/10000\n","194/194 [==============================] - ETA: 0s - loss: 5248.3291 - mse: 4630.1572 - acc: 0.9263 - mae: 49.0491 - mape: 618.1714\n","Epoch 54: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 73s 375ms/step - loss: 5248.3291 - mse: 4630.1572 - acc: 0.9263 - mae: 49.0491 - mape: 618.1714 - val_loss: 6618.6367 - val_mse: 5393.7856 - val_acc: 0.6700 - val_mae: 54.5085 - val_mape: 1224.8517 - lr: 0.0089\n","Epoch 55/10000\n","194/194 [==============================] - ETA: 0s - loss: 5084.5728 - mse: 4465.0337 - acc: 0.9175 - mae: 48.4753 - mape: 619.5389\n","Epoch 55: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 72s 370ms/step - loss: 5084.5728 - mse: 4465.0337 - acc: 0.9175 - mae: 48.4753 - mape: 619.5389 - val_loss: 7213.1313 - val_mse: 5629.0640 - val_acc: 0.6300 - val_mae: 55.7754 - val_mape: 1584.0682 - lr: 0.0087\n","Epoch 56/10000\n","194/194 [==============================] - ETA: 0s - loss: 5296.1206 - mse: 4606.2842 - acc: 0.9294 - mae: 48.5136 - mape: 689.8337\n","Epoch 56: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 72s 373ms/step - loss: 5296.1206 - mse: 4606.2842 - acc: 0.9294 - mae: 48.5136 - mape: 689.8337 - val_loss: 6002.8989 - val_mse: 5495.8145 - val_acc: 0.6400 - val_mae: 57.4031 - val_mape: 507.0839 - lr: 0.0085\n","Epoch 57/10000\n","194/194 [==============================] - ETA: 0s - loss: 5894.9883 - mse: 4960.8433 - acc: 0.9134 - mae: 50.1732 - mape: 934.1440\n","Epoch 57: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 74s 384ms/step - loss: 5894.9883 - mse: 4960.8433 - acc: 0.9134 - mae: 50.1732 - mape: 934.1440 - val_loss: 4944.0625 - val_mse: 4247.3149 - val_acc: 0.5700 - val_mae: 49.3889 - val_mape: 696.7479 - lr: 0.0082\n","Epoch 58/10000\n","194/194 [==============================] - ETA: 0s - loss: 5523.3618 - mse: 4741.7217 - acc: 0.9227 - mae: 48.9732 - mape: 781.6422\n","Epoch 58: val_mse did not improve from 3490.17822\n","\n","Epoch 58: ReduceLROnPlateau reducing learning rate to 0.007897409303113818.\n","194/194 [==============================] - 74s 379ms/step - loss: 5523.3618 - mse: 4741.7217 - acc: 0.9227 - mae: 48.9732 - mape: 781.6422 - val_loss: 257024835584.0000 - val_mse: 257024753664.0000 - val_acc: 0.6100 - val_mae: 115747.1016 - val_mape: 67888.1484 - lr: 0.0080\n","Epoch 59/10000\n","194/194 [==============================] - ETA: 0s - loss: 5204.3462 - mse: 4580.8799 - acc: 0.9191 - mae: 48.5954 - mape: 623.4656\n","Epoch 59: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 74s 381ms/step - loss: 5204.3462 - mse: 4580.8799 - acc: 0.9191 - mae: 48.5954 - mape: 623.4656 - val_loss: 6232.6719 - val_mse: 5412.1562 - val_acc: 0.6400 - val_mae: 56.1450 - val_mape: 820.5162 - lr: 0.0077\n","Epoch 60/10000\n","194/194 [==============================] - ETA: 0s - loss: 4579.9785 - mse: 4112.7490 - acc: 0.9258 - mae: 47.2986 - mape: 467.2299\n","Epoch 60: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 74s 383ms/step - loss: 4579.9785 - mse: 4112.7490 - acc: 0.9258 - mae: 47.2986 - mape: 467.2299 - val_loss: 5217.7495 - val_mse: 4372.0830 - val_acc: 0.4800 - val_mae: 50.5636 - val_mape: 845.6669 - lr: 0.0074\n","Epoch 61/10000\n","194/194 [==============================] - ETA: 0s - loss: 5473.2783 - mse: 4705.5703 - acc: 0.9242 - mae: 49.0024 - mape: 767.7079\n","Epoch 61: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 74s 380ms/step - loss: 5473.2783 - mse: 4705.5703 - acc: 0.9242 - mae: 49.0024 - mape: 767.7079 - val_loss: 21937585739758829568.0000 - val_mse: 21937585739758829568.0000 - val_acc: 0.6500 - val_mae: 1228276096.0000 - val_mape: 5000575488.0000 - lr: 0.0072\n","Epoch 62/10000\n","194/194 [==============================] - ETA: 0s - loss: 5470.5469 - mse: 4786.7290 - acc: 0.9284 - mae: 49.3838 - mape: 683.8199\n","Epoch 62: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 75s 388ms/step - loss: 5470.5469 - mse: 4786.7290 - acc: 0.9284 - mae: 49.3838 - mape: 683.8199 - val_loss: 7692.5713 - val_mse: 7395.6167 - val_acc: 0.6400 - val_mae: 70.0243 - val_mape: 296.9547 - lr: 0.0069\n","Epoch 63/10000\n","194/194 [==============================] - ETA: 0s - loss: 5413.4976 - mse: 4728.1465 - acc: 0.9206 - mae: 49.0740 - mape: 685.3518\n","Epoch 63: val_mse did not improve from 3490.17822\n","\n","Epoch 63: ReduceLROnPlateau reducing learning rate to 0.006508874748833477.\n","194/194 [==============================] - 75s 386ms/step - loss: 5413.4976 - mse: 4728.1465 - acc: 0.9206 - mae: 49.0740 - mape: 685.3518 - val_loss: 5279.9058 - val_mse: 4204.6670 - val_acc: 0.4900 - val_mae: 47.7750 - val_mape: 1075.2382 - lr: 0.0066\n","Epoch 64/10000\n","194/194 [==============================] - ETA: 0s - loss: 5589.6230 - mse: 4872.7280 - acc: 0.9263 - mae: 50.0235 - mape: 716.8978\n","Epoch 64: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 75s 386ms/step - loss: 5589.6230 - mse: 4872.7280 - acc: 0.9263 - mae: 50.0235 - mape: 716.8978 - val_loss: 6203.5420 - val_mse: 5958.8501 - val_acc: 0.6400 - val_mae: 59.2615 - val_mape: 244.6908 - lr: 0.0063\n","Epoch 65/10000\n","194/194 [==============================] - ETA: 0s - loss: 5468.8760 - mse: 4643.3491 - acc: 0.9191 - mae: 48.4934 - mape: 825.5288\n","Epoch 65: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 76s 392ms/step - loss: 5468.8760 - mse: 4643.3491 - acc: 0.9191 - mae: 48.4934 - mape: 825.5288 - val_loss: 5350.3374 - val_mse: 4499.1934 - val_acc: 0.6200 - val_mae: 50.8680 - val_mape: 851.1445 - lr: 0.0060\n","Epoch 66/10000\n","194/194 [==============================] - ETA: 0s - loss: 5089.0386 - mse: 4469.7891 - acc: 0.9278 - mae: 48.3390 - mape: 619.2502\n","Epoch 66: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 77s 397ms/step - loss: 5089.0386 - mse: 4469.7891 - acc: 0.9278 - mae: 48.3390 - mape: 619.2502 - val_loss: 7025.9849 - val_mse: 5478.2476 - val_acc: 0.5900 - val_mae: 53.2854 - val_mape: 1547.7378 - lr: 0.0056\n","Epoch 67/10000\n","194/194 [==============================] - ETA: 0s - loss: 5864.6929 - mse: 4978.2402 - acc: 0.9237 - mae: 49.8302 - mape: 886.4542\n","Epoch 67: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 75s 385ms/step - loss: 5864.6929 - mse: 4978.2402 - acc: 0.9237 - mae: 49.8302 - mape: 886.4542 - val_loss: 8492.0049 - val_mse: 7811.0850 - val_acc: 0.5800 - val_mae: 68.1726 - val_mape: 680.9199 - lr: 0.0053\n","Epoch 68/10000\n","194/194 [==============================] - ETA: 0s - loss: 4703.4639 - mse: 4164.5669 - acc: 0.9186 - mae: 46.7256 - mape: 538.8977\n","Epoch 68: val_mse did not improve from 3490.17822\n","\n","Epoch 68: ReduceLROnPlateau reducing learning rate to 0.004964577779173851.\n","194/194 [==============================] - 75s 386ms/step - loss: 4703.4639 - mse: 4164.5669 - acc: 0.9186 - mae: 46.7256 - mape: 538.8977 - val_loss: 4775.7095 - val_mse: 4251.5244 - val_acc: 0.6800 - val_mae: 50.9995 - val_mape: 524.1850 - lr: 0.0050\n","Epoch 69/10000\n","194/194 [==============================] - ETA: 0s - loss: 5002.7373 - mse: 4353.4297 - acc: 0.9423 - mae: 47.0999 - mape: 649.3074\n","Epoch 69: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 77s 397ms/step - loss: 5002.7373 - mse: 4353.4297 - acc: 0.9423 - mae: 47.0999 - mape: 649.3074 - val_loss: 5042.2305 - val_mse: 4209.5908 - val_acc: 0.5700 - val_mae: 48.7413 - val_mape: 832.6395 - lr: 0.0047\n","Epoch 70/10000\n","194/194 [==============================] - ETA: 0s - loss: 4936.4751 - mse: 4343.8213 - acc: 0.9232 - mae: 47.8138 - mape: 592.6541\n","Epoch 70: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 76s 394ms/step - loss: 4936.4751 - mse: 4343.8213 - acc: 0.9232 - mae: 47.8138 - mape: 592.6541 - val_loss: 6229.3931 - val_mse: 4825.6870 - val_acc: 0.6200 - val_mae: 51.7519 - val_mape: 1403.7059 - lr: 0.0044\n","Epoch 71/10000\n","194/194 [==============================] - ETA: 0s - loss: 5333.0869 - mse: 4610.6943 - acc: 0.9320 - mae: 48.0316 - mape: 722.3932\n","Epoch 71: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 75s 386ms/step - loss: 5333.0869 - mse: 4610.6943 - acc: 0.9320 - mae: 48.0316 - mape: 722.3932 - val_loss: 4852.5518 - val_mse: 4338.5767 - val_acc: 0.5700 - val_mae: 50.9752 - val_mape: 513.9745 - lr: 0.0041\n","Epoch 72/10000\n","194/194 [==============================] - ETA: 0s - loss: 5081.8525 - mse: 4470.7070 - acc: 0.9247 - mae: 48.3013 - mape: 611.1471\n","Epoch 72: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 77s 396ms/step - loss: 5081.8525 - mse: 4470.7070 - acc: 0.9247 - mae: 48.3013 - mape: 611.1471 - val_loss: 6790.3018 - val_mse: 5245.0073 - val_acc: 0.6100 - val_mae: 53.3892 - val_mape: 1545.2946 - lr: 0.0038\n","Epoch 73/10000\n","194/194 [==============================] - ETA: 0s - loss: 5291.1943 - mse: 4560.4780 - acc: 0.9196 - mae: 47.8267 - mape: 730.7181\n","Epoch 73: val_mse did not improve from 3490.17822\n","\n","Epoch 73: ReduceLROnPlateau reducing learning rate to 0.003418824726250023.\n","194/194 [==============================] - 78s 402ms/step - loss: 5291.1943 - mse: 4560.4780 - acc: 0.9196 - mae: 47.8267 - mape: 730.7181 - val_loss: 6322.4805 - val_mse: 4804.0913 - val_acc: 0.5600 - val_mae: 51.4315 - val_mape: 1518.3894 - lr: 0.0035\n","Epoch 74/10000\n","194/194 [==============================] - ETA: 0s - loss: 5481.9146 - mse: 4599.1689 - acc: 0.9160 - mae: 47.4566 - mape: 882.7458\n","Epoch 74: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 76s 390ms/step - loss: 5481.9146 - mse: 4599.1689 - acc: 0.9160 - mae: 47.4566 - mape: 882.7458 - val_loss: 3731.9441 - val_mse: 3696.2363 - val_acc: 0.6000 - val_mae: 47.0695 - val_mape: 35.7080 - lr: 0.0032\n","Epoch 75/10000\n","194/194 [==============================] - ETA: 0s - loss: 4595.9287 - mse: 4107.4673 - acc: 0.9366 - mae: 46.1907 - mape: 488.4611\n","Epoch 75: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 77s 396ms/step - loss: 4595.9287 - mse: 4107.4673 - acc: 0.9366 - mae: 46.1907 - mape: 488.4611 - val_loss: 6094.9326 - val_mse: 5131.0776 - val_acc: 0.6100 - val_mae: 52.9654 - val_mape: 963.8551 - lr: 0.0029\n","Epoch 76/10000\n","194/194 [==============================] - ETA: 0s - loss: 5030.1201 - mse: 4384.0024 - acc: 0.9206 - mae: 47.8425 - mape: 646.1180\n","Epoch 76: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 78s 400ms/step - loss: 5030.1201 - mse: 4384.0024 - acc: 0.9206 - mae: 47.8425 - mape: 646.1180 - val_loss: 6602.0718 - val_mse: 5276.1406 - val_acc: 0.5800 - val_mae: 54.2608 - val_mape: 1325.9320 - lr: 0.0026\n","Epoch 77/10000\n","194/194 [==============================] - ETA: 0s - loss: 4996.9341 - mse: 4512.3501 - acc: 0.9345 - mae: 48.7774 - mape: 484.5849\n","Epoch 77: val_mse did not improve from 3490.17822\n","194/194 [==============================] - 76s 392ms/step - loss: 4996.9341 - mse: 4512.3501 - acc: 0.9345 - mae: 48.7774 - mape: 484.5849 - val_loss: 4367.4136 - val_mse: 4091.6426 - val_acc: 0.5900 - val_mae: 48.0293 - val_mape: 275.7714 - lr: 0.0023\n","Epoch 78/10000\n","194/194 [==============================] - ETA: 0s - loss: 4716.4341 - mse: 4198.4956 - acc: 0.9443 - mae: 46.8326 - mape: 517.9380\n","Epoch 78: val_mse improved from 3490.17822 to 3415.66724, saving model to ./data/model/DNN_Sales_model.h5\n","194/194 [==============================] - 78s 405ms/step - loss: 4716.4341 - mse: 4198.4956 - acc: 0.9443 - mae: 46.8326 - mape: 517.9380 - val_loss: 3663.7847 - val_mse: 3415.6672 - val_acc: 0.6000 - val_mae: 45.1839 - val_mape: 248.1175 - lr: 0.0020\n","Epoch 79/10000\n","194/194 [==============================] - ETA: 0s - loss: 4995.6211 - mse: 4368.7778 - acc: 0.9371 - mae: 47.1209 - mape: 626.8416\n","Epoch 79: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 78s 400ms/step - loss: 4995.6211 - mse: 4368.7778 - acc: 0.9371 - mae: 47.1209 - mape: 626.8416 - val_loss: 4961.6660 - val_mse: 4309.4775 - val_acc: 0.6400 - val_mae: 48.7941 - val_mape: 652.1888 - lr: 0.0018\n","Epoch 80/10000\n","194/194 [==============================] - ETA: 0s - loss: 5264.5933 - mse: 4529.5679 - acc: 0.9206 - mae: 48.0378 - mape: 735.0247\n","Epoch 80: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 78s 404ms/step - loss: 5264.5933 - mse: 4529.5679 - acc: 0.9206 - mae: 48.0378 - mape: 735.0247 - val_loss: 5332.2485 - val_mse: 4863.9414 - val_acc: 0.6000 - val_mae: 52.4840 - val_mape: 468.3078 - lr: 0.0016\n","Epoch 81/10000\n","194/194 [==============================] - ETA: 0s - loss: 5075.7500 - mse: 4434.8135 - acc: 0.9268 - mae: 47.6614 - mape: 640.9383\n","Epoch 81: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 79s 406ms/step - loss: 5075.7500 - mse: 4434.8135 - acc: 0.9268 - mae: 47.6614 - mape: 640.9383 - val_loss: 5402.9492 - val_mse: 4214.9224 - val_acc: 0.6800 - val_mae: 48.5140 - val_mape: 1188.0270 - lr: 0.0013\n","Epoch 82/10000\n","194/194 [==============================] - ETA: 0s - loss: 4833.2061 - mse: 4300.9707 - acc: 0.9268 - mae: 46.9963 - mape: 532.2344\n","Epoch 82: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 78s 401ms/step - loss: 4833.2061 - mse: 4300.9707 - acc: 0.9268 - mae: 46.9963 - mape: 532.2344 - val_loss: 6069.6587 - val_mse: 4881.1255 - val_acc: 0.6600 - val_mae: 51.3712 - val_mape: 1188.5325 - lr: 0.0011\n","Epoch 83/10000\n","194/194 [==============================] - ETA: 0s - loss: 4939.5620 - mse: 4424.7930 - acc: 0.9356 - mae: 47.3498 - mape: 514.7698\n","Epoch 83: val_mse did not improve from 3415.66724\n","\n","Epoch 83: ReduceLROnPlateau reducing learning rate to 0.000925466365297325.\n","194/194 [==============================] - 79s 408ms/step - loss: 4939.5620 - mse: 4424.7930 - acc: 0.9356 - mae: 47.3498 - mape: 514.7698 - val_loss: 4468.8936 - val_mse: 3682.7117 - val_acc: 0.6300 - val_mae: 45.1607 - val_mape: 786.1817 - lr: 9.3481e-04\n","Epoch 84/10000\n","194/194 [==============================] - ETA: 0s - loss: 5204.7041 - mse: 4462.3970 - acc: 0.9216 - mae: 47.4037 - mape: 742.3061\n","Epoch 84: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 78s 402ms/step - loss: 5204.7041 - mse: 4462.3970 - acc: 0.9216 - mae: 47.4037 - mape: 742.3061 - val_loss: 4476.2446 - val_mse: 3893.9478 - val_acc: 0.6300 - val_mae: 46.5427 - val_mape: 582.2972 - lr: 7.5832e-04\n","Epoch 85/10000\n","194/194 [==============================] - ETA: 0s - loss: 4521.1875 - mse: 4004.2534 - acc: 0.9299 - mae: 45.6793 - mape: 516.9331\n","Epoch 85: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 78s 404ms/step - loss: 4521.1875 - mse: 4004.2534 - acc: 0.9299 - mae: 45.6793 - mape: 516.9331 - val_loss: 5114.4077 - val_mse: 4594.7837 - val_acc: 0.6700 - val_mae: 50.3319 - val_mape: 519.6244 - lr: 5.9891e-04\n","Epoch 86/10000\n","194/194 [==============================] - ETA: 0s - loss: 5039.2695 - mse: 4462.0093 - acc: 0.9387 - mae: 47.1600 - mape: 577.2615\n","Epoch 86: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 79s 407ms/step - loss: 5039.2695 - mse: 4462.0093 - acc: 0.9387 - mae: 47.1600 - mape: 577.2615 - val_loss: 7691.8345 - val_mse: 5948.2329 - val_acc: 0.5800 - val_mae: 54.7747 - val_mape: 1743.6019 - lr: 4.5723e-04\n","Epoch 87/10000\n","194/194 [==============================] - ETA: 0s - loss: 5145.2012 - mse: 4469.7676 - acc: 0.9273 - mae: 47.4995 - mape: 675.4344\n","Epoch 87: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 79s 408ms/step - loss: 5145.2012 - mse: 4469.7676 - acc: 0.9273 - mae: 47.4995 - mape: 675.4344 - val_loss: 5136.6064 - val_mse: 4173.6816 - val_acc: 0.6500 - val_mae: 47.0400 - val_mape: 962.9246 - lr: 3.3386e-04\n","Epoch 88/10000\n","194/194 [==============================] - ETA: 0s - loss: 5100.1377 - mse: 4407.0796 - acc: 0.9314 - mae: 47.2671 - mape: 693.0594\n","Epoch 88: val_mse did not improve from 3415.66724\n","\n","Epoch 88: ReduceLROnPlateau reducing learning rate to 0.00022699515771819277.\n","194/194 [==============================] - 81s 417ms/step - loss: 5100.1377 - mse: 4407.0796 - acc: 0.9314 - mae: 47.2671 - mape: 693.0594 - val_loss: 5895.6279 - val_mse: 4650.5151 - val_acc: 0.6100 - val_mae: 50.1615 - val_mape: 1245.1132 - lr: 2.2929e-04\n","Epoch 89/10000\n","194/194 [==============================] - ETA: 0s - loss: 5084.4321 - mse: 4436.9990 - acc: 0.9304 - mae: 47.4498 - mape: 647.4343\n","Epoch 89: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 80s 412ms/step - loss: 5084.4321 - mse: 4436.9990 - acc: 0.9304 - mae: 47.4498 - mape: 647.4343 - val_loss: 3559.6462 - val_mse: 3524.5334 - val_acc: 0.6600 - val_mae: 46.2590 - val_mape: 35.1126 - lr: 1.4394e-04\n","Epoch 90/10000\n","194/194 [==============================] - ETA: 0s - loss: 4603.8247 - mse: 4165.2378 - acc: 0.9216 - mae: 46.8715 - mape: 438.5861\n","Epoch 90: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 80s 412ms/step - loss: 4603.8247 - mse: 4165.2378 - acc: 0.9216 - mae: 46.8715 - mape: 438.5861 - val_loss: 6448.5869 - val_mse: 5418.5361 - val_acc: 0.6100 - val_mae: 53.8128 - val_mape: 1030.0505 - lr: 7.8155e-05\n","Epoch 91/10000\n","194/194 [==============================] - ETA: 0s - loss: 4698.7842 - mse: 4169.1606 - acc: 0.9330 - mae: 46.6763 - mape: 529.6215\n","Epoch 91: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 79s 406ms/step - loss: 4698.7842 - mse: 4169.1606 - acc: 0.9330 - mae: 46.6763 - mape: 529.6215 - val_loss: 4147.5190 - val_mse: 3588.5657 - val_acc: 0.5600 - val_mae: 45.9739 - val_mape: 558.9532 - lr: 3.2202e-05\n","Epoch 92/10000\n","194/194 [==============================] - ETA: 0s - loss: 4949.4941 - mse: 4332.6328 - acc: 0.9320 - mae: 47.3006 - mape: 616.8626\n","Epoch 92: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 81s 420ms/step - loss: 4949.4941 - mse: 4332.6328 - acc: 0.9320 - mae: 47.3006 - mape: 616.8626 - val_loss: 5556.2358 - val_mse: 4535.8794 - val_acc: 0.5900 - val_mae: 50.3808 - val_mape: 1020.3566 - lr: 6.2659e-06\n","Epoch 93/10000\n","194/194 [==============================] - ETA: 0s - loss: 5171.4189 - mse: 4482.5273 - acc: 0.9227 - mae: 47.6833 - mape: 688.8922\n","Epoch 93: val_mse did not improve from 3415.66724\n","\n","Epoch 93: ReduceLROnPlateau reducing learning rate to 0.009899888215586542.\n","194/194 [==============================] - 81s 416ms/step - loss: 5171.4189 - mse: 4482.5273 - acc: 0.9227 - mae: 47.6833 - mape: 688.8922 - val_loss: 28383.7695 - val_mse: 26310.1191 - val_acc: 0.3700 - val_mae: 117.9063 - val_mape: 2073.6521 - lr: 0.0100\n","Epoch 94/10000\n","194/194 [==============================] - ETA: 0s - loss: 5047.1313 - mse: 4468.6782 - acc: 0.9340 - mae: 48.4165 - mape: 578.4514\n","Epoch 94: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 81s 418ms/step - loss: 5047.1313 - mse: 4468.6782 - acc: 0.9340 - mae: 48.4165 - mape: 578.4514 - val_loss: 56582573467369472.0000 - val_mse: 56582573467369472.0000 - val_acc: 0.2800 - val_mae: 106871080.0000 - val_mape: 51165164.0000 - lr: 0.0100\n","Epoch 95/10000\n","194/194 [==============================] - ETA: 0s - loss: 5299.1660 - mse: 4636.4590 - acc: 0.9211 - mae: 49.4591 - mape: 662.7064\n","Epoch 95: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 81s 417ms/step - loss: 5299.1660 - mse: 4636.4590 - acc: 0.9211 - mae: 49.4591 - mape: 662.7064 - val_loss: 737304872356937728.0000 - val_mse: 737304872356937728.0000 - val_acc: 0.3000 - val_mae: 313792192.0000 - val_mape: 1087880320.0000 - lr: 0.0100\n","Epoch 96/10000\n","194/194 [==============================] - ETA: 0s - loss: 5406.8330 - mse: 4772.5635 - acc: 0.9216 - mae: 50.0515 - mape: 634.2699\n","Epoch 96: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 83s 425ms/step - loss: 5406.8330 - mse: 4772.5635 - acc: 0.9216 - mae: 50.0515 - mape: 634.2699 - val_loss: 9809.5342 - val_mse: 8238.3945 - val_acc: 0.6800 - val_mae: 66.4436 - val_mape: 1571.1390 - lr: 0.0100\n","Epoch 97/10000\n","194/194 [==============================] - ETA: 0s - loss: 5418.2983 - mse: 4806.7090 - acc: 0.9144 - mae: 50.0947 - mape: 611.5925\n","Epoch 97: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 82s 421ms/step - loss: 5418.2983 - mse: 4806.7090 - acc: 0.9144 - mae: 50.0947 - mape: 611.5925 - val_loss: 6683.3877 - val_mse: 4869.6177 - val_acc: 0.5100 - val_mae: 52.1425 - val_mape: 1813.7701 - lr: 0.0100\n","Epoch 98/10000\n","194/194 [==============================] - ETA: 0s - loss: 5249.1919 - mse: 4589.3032 - acc: 0.9299 - mae: 48.4317 - mape: 659.8882\n","Epoch 98: val_mse did not improve from 3415.66724\n","\n","Epoch 98: ReduceLROnPlateau reducing learning rate to 0.009832425713539124.\n","194/194 [==============================] - 82s 422ms/step - loss: 5249.1919 - mse: 4589.3032 - acc: 0.9299 - mae: 48.4317 - mape: 659.8882 - val_loss: 8867.7734 - val_mse: 8052.7661 - val_acc: 0.5800 - val_mae: 69.8671 - val_mape: 815.0067 - lr: 0.0099\n","Epoch 99/10000\n","194/194 [==============================] - ETA: 0s - loss: 5271.1699 - mse: 4617.2749 - acc: 0.9258 - mae: 49.2792 - mape: 653.8980\n","Epoch 99: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 82s 424ms/step - loss: 5271.1699 - mse: 4617.2749 - acc: 0.9258 - mae: 49.2792 - mape: 653.8980 - val_loss: 9157.6689 - val_mse: 8893.2559 - val_acc: 0.6200 - val_mae: 73.0339 - val_mape: 264.4120 - lr: 0.0099\n","Epoch 100/10000\n","194/194 [==============================] - ETA: 0s - loss: 5133.7998 - mse: 4603.1060 - acc: 0.9320 - mae: 48.9589 - mape: 530.6940\n","Epoch 100: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 82s 425ms/step - loss: 5133.7998 - mse: 4603.1060 - acc: 0.9320 - mae: 48.9589 - mape: 530.6940 - val_loss: 6665.0151 - val_mse: 5031.8906 - val_acc: 0.6600 - val_mae: 52.3650 - val_mape: 1633.1250 - lr: 0.0099\n","Epoch 101/10000\n","194/194 [==============================] - ETA: 0s - loss: 5787.5610 - mse: 5072.3984 - acc: 0.9222 - mae: 51.2248 - mape: 715.1580\n","Epoch 101: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 83s 427ms/step - loss: 5787.5610 - mse: 5072.3984 - acc: 0.9222 - mae: 51.2248 - mape: 715.1580 - val_loss: 9928.3203 - val_mse: 8878.4961 - val_acc: 0.5400 - val_mae: 79.0666 - val_mape: 1049.8232 - lr: 0.0098\n","Epoch 102/10000\n","194/194 [==============================] - ETA: 0s - loss: 5558.5625 - mse: 4823.8696 - acc: 0.9309 - mae: 50.0598 - mape: 734.6917\n","Epoch 102: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 84s 432ms/step - loss: 5558.5625 - mse: 4823.8696 - acc: 0.9309 - mae: 50.0598 - mape: 734.6917 - val_loss: 8752384180224.0000 - val_mse: 8752384180224.0000 - val_acc: 0.5900 - val_mae: 924937.1250 - val_mape: 512060.2500 - lr: 0.0098\n","Epoch 103/10000\n","194/194 [==============================] - ETA: 0s - loss: 4642.9048 - mse: 4162.4712 - acc: 0.9268 - mae: 46.9380 - mape: 480.4337\n","Epoch 103: val_mse did not improve from 3415.66724\n","\n","Epoch 103: ReduceLROnPlateau reducing learning rate to 0.009642228092998266.\n","194/194 [==============================] - 84s 433ms/step - loss: 4642.9048 - mse: 4162.4712 - acc: 0.9268 - mae: 46.9380 - mape: 480.4337 - val_loss: 977156263903232.0000 - val_mse: 977156263903232.0000 - val_acc: 0.4400 - val_mae: 9008794.0000 - val_mape: 26696648.0000 - lr: 0.0097\n","Epoch 104/10000\n","194/194 [==============================] - ETA: 0s - loss: 5284.4478 - mse: 4659.2334 - acc: 0.9309 - mae: 48.9846 - mape: 625.2141\n","Epoch 104: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 83s 430ms/step - loss: 5284.4478 - mse: 4659.2334 - acc: 0.9309 - mae: 48.9846 - mape: 625.2141 - val_loss: 37913785150734336.0000 - val_mse: 37913785150734336.0000 - val_acc: 0.4100 - val_mae: 73400952.0000 - val_mape: 1224228736.0000 - lr: 0.0097\n","Epoch 105/10000\n","194/194 [==============================] - ETA: 0s - loss: 5211.4985 - mse: 4615.7651 - acc: 0.9211 - mae: 48.6417 - mape: 595.7352\n","Epoch 105: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 83s 430ms/step - loss: 5211.4985 - mse: 4615.7651 - acc: 0.9211 - mae: 48.6417 - mape: 595.7352 - val_loss: 7865.5483 - val_mse: 6971.4639 - val_acc: 0.5600 - val_mae: 61.0947 - val_mape: 894.0847 - lr: 0.0096\n","Epoch 106/10000\n","194/194 [==============================] - ETA: 0s - loss: 5458.2749 - mse: 4808.6074 - acc: 0.9289 - mae: 49.4021 - mape: 649.6682\n","Epoch 106: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 86s 443ms/step - loss: 5458.2749 - mse: 4808.6074 - acc: 0.9289 - mae: 49.4021 - mape: 649.6682 - val_loss: 686696001397850112.0000 - val_mse: 686696001397850112.0000 - val_acc: 0.5300 - val_mae: 301824320.0000 - val_mape: 2095097600.0000 - lr: 0.0096\n","Epoch 107/10000\n","194/194 [==============================] - ETA: 0s - loss: 5103.3652 - mse: 4474.7744 - acc: 0.9232 - mae: 48.1867 - mape: 628.5936\n","Epoch 107: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 84s 434ms/step - loss: 5103.3652 - mse: 4474.7744 - acc: 0.9232 - mae: 48.1867 - mape: 628.5936 - val_loss: 156028813443072.0000 - val_mse: 156028813443072.0000 - val_acc: 0.6900 - val_mae: 1970968.3750 - val_mape: 1044121.6875 - lr: 0.0095\n","Epoch 108/10000\n","194/194 [==============================] - ETA: 0s - loss: 5038.2070 - mse: 4451.5186 - acc: 0.9299 - mae: 48.2482 - mape: 586.6865\n","Epoch 108: val_mse did not improve from 3415.66724\n","\n","Epoch 108: ReduceLROnPlateau reducing learning rate to 0.009334077816456557.\n","194/194 [==============================] - 85s 441ms/step - loss: 5038.2070 - mse: 4451.5186 - acc: 0.9299 - mae: 48.2482 - mape: 586.6865 - val_loss: 21617.9219 - val_mse: 20104.7441 - val_acc: 0.5500 - val_mae: 105.6108 - val_mape: 1513.1781 - lr: 0.0094\n","Epoch 109/10000\n","194/194 [==============================] - ETA: 0s - loss: 5300.8789 - mse: 4673.0923 - acc: 0.9242 - mae: 49.3000 - mape: 627.7863\n","Epoch 109: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 84s 435ms/step - loss: 5300.8789 - mse: 4673.0923 - acc: 0.9242 - mae: 49.3000 - mape: 627.7863 - val_loss: 12607244415660982272.0000 - val_mse: 12607244415660982272.0000 - val_acc: 0.5100 - val_mae: 1222137472.0000 - val_mape: 13664925696.0000 - lr: 0.0094\n","Epoch 110/10000\n","194/194 [==============================] - ETA: 0s - loss: 5231.6909 - mse: 4598.8467 - acc: 0.9242 - mae: 48.5323 - mape: 632.8418\n","Epoch 110: val_mse did not improve from 3415.66724\n","194/194 [==============================] - 85s 437ms/step - loss: 5231.6909 - mse: 4598.8467 - acc: 0.9242 - mae: 48.5323 - mape: 632.8418 - val_loss: 11061.2012 - val_mse: 10884.5801 - val_acc: 0.6300 - val_mae: 82.5032 - val_mape: 176.6225 - lr: 0.0093\n"]}],"source":["EPOCHS = 10000\n","steps_per_epoch = int((num_train/batch_size))\n","\n","try:\n","  Sales_model.load_weights(os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model.{save_format}'))\n","  Sales_model.save_weights(os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model'), save_format=save_format)\n","  # Sales_model.set_weights(load_model(os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model'), {\"MeanExponentWeightedError\":MeanExponentWeightedError()}).get_weights())\n","  # Sales_model.save(os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model'), save_format=save_format)\n","  # Sales_model = load_model(os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model'), {\"MeanExponentWeightedError\":MeanExponentWeightedError()}, compile=True)\n","  print(\"Checkpoint Loaded\")\n","except Exception as error:\n","  print(\"Error trying to load checkpoint.\")\n","  print(error)\n","                     \n","\n","history, Sales_model = train_model(resume=False, epochs=EPOCHS, initial_epoch=0, batch_size=batch_size, model=Sales_model, savepath=path_checkpoint)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QA_DHbQ1nbkn"},"outputs":[],"source":["# plot_train_history(history, 'Model Training History')\n","_ = pd.DataFrame(Sales_model.beta_values, columns=[f'β_{i+1}' for i in range(len(Sales_model.beta_values[0]))]).plot(subplots=True, figsize=(25,30), title=('Parametric SILU Parameters Across Steps') )"]},{"cell_type":"markdown","metadata":{"id":"GfBpgoUVnbko"},"source":["### Load Checkpoint\n","\n","Because we use early-stopping when training the model, it is possible that the model's performance has worsened on the test-set for several epochs before training was stopped. We therefore reload the last saved checkpoint, which should have the best performance on the test-set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jocf4vjNnbko"},"outputs":[],"source":["with tf.device('/device:GPU:0'):\n","    try:\n","        Sales_model.load_weights(os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model.{save_format}'))\n","        # Sales_model = load_model(os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model'), {\"MeanExponentWeightedError\":MeanExponentWeightedError()})\n","        # Sales_model.set_weights(load_model(os.path.join(DATA_FOLDER, f'model/{MODEL}_Sales_model'), {\"MeanExponentWeightedError\":MeanExponentWeightedError()}).get_weights())\n","        print(\"Checkpoint Loaded\")\n","    except Exception as error:\n","        print(\"Error trying to load checkpoint.\")\n","        print(error)"]},{"cell_type":"markdown","metadata":{"id":"5lcYcUcbnbko"},"source":["## Performance on Test-Set\n","\n","We can now evaluate the model's performance on the test-set. This function expects a batch of data, but we will just use one long time-series for the test-set, so we just expand the array-dimensionality to create a batch with that one sequence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAbqSrPsnbko"},"outputs":[],"source":["with tf.device('/device:GPU:0'):\n","    Sales_model.evaluate(x_train_generator, steps=train_validation_steps)\n","    Sales_model.evaluate(x_val_generator, steps=train_validation_steps)\n","    Sales_model.evaluate(x_test_generator, steps=test_validation_steps)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":184,"status":"ok","timestamp":1677211942088,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"k6I_wxUNnbko"},"outputs":[],"source":["import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Conv2D, Conv2DTranspose\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":222,"status":"ok","timestamp":1677211944349,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"92sQW_V3nbko"},"outputs":[],"source":["# Model configuration\n","img_width, img_height = 28, 28\n","batch_size = 1000\n","no_epochs = 25\n","no_classes = 10\n","validation_split = 0.2\n","verbosity = 1"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2454,"status":"ok","timestamp":1677211947114,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"mUm5uDAinbkp","outputId":"4077fe1f-bd90-44e5-ccd1-ecae3319a7e5"},"outputs":[],"source":["# Load MNIST dataset\n","(input_train, target_train), (input_test, target_test) = mnist.load_data()\n","\n","# Reshape data\n","input_train = input_train.reshape(input_train.shape[0], img_width, img_height, 1)\n","input_test = input_test.reshape(input_test.shape[0], img_width, img_height, 1)\n","input_shape = (img_width, img_height, 1)\n","\n","# Parse numbers as floats\n","input_train = input_train.astype('float32')\n","input_test = input_test.astype('float32')\n","\n","# Normalize data\n","input_train = input_train / 255\n","input_test = input_test / 255"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1217,"status":"ok","timestamp":1677211948523,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"n0G6UtySnbkp"},"outputs":[],"source":["# Create the model\n","model = Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal', input_shape=input_shape))\n","model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))\n","model.add(Conv2D(8, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal'))\n","model.add(Conv2DTranspose(8, kernel_size=(3,3), activation='relu', kernel_initializer='he_normal'))\n","model.add(Conv2DTranspose(16, kernel_size=(3,3), activation='relu', kernel_initializer='he_normal'))\n","model.add(Conv2DTranspose(32, kernel_size=(3,3), activation='relu', kernel_initializer='he_normal'))\n","model.add(Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same'))"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677211948523,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"IkhKDt3snbkp"},"outputs":[],"source":["# Compile and fit data\n","model.compile(optimizer='adam', loss='binary_crossentropy')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"elapsed":21760,"status":"error","timestamp":1677211970280,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"htG0EVEEnbkp","outputId":"009e8aaf-4230-4b7a-b317-f48d064e8949"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCanceled future for execute_request message before replies were done"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["model.fit(input_train, input_train,\n","                epochs=no_epochs,\n","                batch_size=batch_size,\n","                validation_split=validation_split, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1677211970281,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"drmAhReHnbkq"},"outputs":[],"source":["# Generate reconstructions\n","num_reconstructions = 8\n","samples = input_test[:num_reconstructions]\n","targets = target_test[:num_reconstructions]\n","reconstructions = model.predict(samples)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21521,"status":"aborted","timestamp":1677211970286,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"gOGnqontnbkq"},"outputs":[],"source":["# Plot reconstructions\n","for i in np.arange(0, num_reconstructions):\n","  # Get the sample and the reconstruction\n","  sample = samples[i][:, :, 0]\n","  reconstruction = reconstructions[i][:, :, 0]\n","  input_class = targets[i]\n","  # Matplotlib preparations\n","  fig, axes = plt.subplots(1, 2)\n","  # Plot sample and reconstruciton\n","  axes[0].imshow(sample)\n","  axes[0].set_title('Original image')\n","  axes[1].imshow(reconstruction)\n","  axes[1].set_title('Reconstruction with Conv2DTranspose')\n","  fig.suptitle(f'MNIST target = {input_class}')\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20399,"status":"aborted","timestamp":1677211970286,"user":{"displayName":"Owusu","userId":"16122779424017637748"},"user_tz":300},"id":"-QbzwFyKnbkq"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"}}},"nbformat":4,"nbformat_minor":0}
